{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.models import Sequential\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States 4\n"
     ]
    }
   ],
   "source": [
    "states = env.observation_space.shape[0]\n",
    "print(\"States\", states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions 2\n"
     ]
    }
   ],
   "source": [
    "actions = env.action_space.n\n",
    "print(\"Actions\", actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1 score 15.0\n",
      "episode 2 score 12.0\n",
      "episode 3 score 9.0\n",
      "episode 4 score 21.0\n",
      "episode 5 score 14.0\n",
      "episode 6 score 27.0\n",
      "episode 7 score 28.0\n",
      "episode 8 score 21.0\n",
      "episode 9 score 10.0\n",
      "episode 10 score 11.0\n"
     ]
    }
   ],
   "source": [
    "episodes = 10\n",
    "for episode in range(1, episodes + 1):\n",
    "    # At each begining reset the game\n",
    "    state = env.reset()\n",
    "    # set done to False\n",
    "    done = False\n",
    "    # set score to 0\n",
    "    score = 0\n",
    "    # while the game is not finished\n",
    "    while not done:\n",
    "        # visualize each step\n",
    "        env.render()\n",
    "        # choose a random action\n",
    "        action = random.choice([0,1])\n",
    "        # execute the action\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        # keep track of rewards\n",
    "        score += reward\n",
    "    print(\"episode {} score {}\".format(episode, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent(states, actions):\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=(1, states)))\n",
    "    model.add(Dense(24, activation=\"relu\"))\n",
    "    model.add(Dense(24, activation=\"relu\"))\n",
    "    model.add(Dense(24, activation=\"relu\"))\n",
    "    model.add(Dense(actions, activation=\"linear\"))\n",
    "    return model\n",
    "\n",
    "\n",
    "model = agent(env.observation_space.shape[0], env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 4)                 0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 24)                120       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 24)                600       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 24)                600       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 2)                 50        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,370\n",
      "Trainable params: 1,370\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.agents import SARSAAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "\n",
    "policy = EpsGreedyQPolicy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarsa = SARSAAgent(model=model, policy=policy, nb_actions=env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarsa.compile(\"adam\", metrics=[\"mse\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 50000 steps ...\n",
      "    10/50000: episode: 1, duration: 0.322s, episode steps: 10, steps per second: 31, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.123 [-2.544, 1.602], \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danie\\anaconda3\\envs\\Daniels\\lib\\site-packages\\rl\\callbacks.py:158: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  metrics = np.array(self.metrics[episode])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    19/50000: episode: 2, duration: 0.655s, episode steps: 9, steps per second: 14, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.157 [-2.823, 1.741], \n",
      "    29/50000: episode: 3, duration: 0.334s, episode steps: 10, steps per second: 30, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.129 [-3.037, 1.984], \n",
      "    38/50000: episode: 4, duration: 0.299s, episode steps: 9, steps per second: 30, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.138 [-2.237, 1.402], \n",
      "    46/50000: episode: 5, duration: 0.269s, episode steps: 8, steps per second: 30, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.183 [-2.603, 1.519], \n",
      "    60/50000: episode: 6, duration: 0.461s, episode steps: 14, steps per second: 30, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.857 [0.000, 1.000], mean observation: -0.100 [-2.950, 1.920], \n",
      "    70/50000: episode: 7, duration: 0.332s, episode steps: 10, steps per second: 30, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.163 [-3.063, 1.912], \n",
      "    81/50000: episode: 8, duration: 0.367s, episode steps: 11, steps per second: 30, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.122 [-2.793, 1.767], \n",
      "    90/50000: episode: 9, duration: 0.301s, episode steps: 9, steps per second: 30, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.139 [-2.830, 1.786], \n",
      "    99/50000: episode: 10, duration: 0.299s, episode steps: 9, steps per second: 30, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.130 [-2.458, 1.588], \n",
      "   107/50000: episode: 11, duration: 0.266s, episode steps: 8, steps per second: 30, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.145 [-2.553, 1.599], \n",
      "   115/50000: episode: 12, duration: 0.265s, episode steps: 8, steps per second: 30, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.146 [-2.509, 1.577], \n",
      "   125/50000: episode: 13, duration: 0.334s, episode steps: 10, steps per second: 30, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.140 [-2.990, 1.903], \n",
      "   136/50000: episode: 14, duration: 0.366s, episode steps: 11, steps per second: 30, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.125 [-2.431, 1.540], \n",
      "   147/50000: episode: 15, duration: 0.366s, episode steps: 11, steps per second: 30, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.138 [-2.759, 1.719], \n",
      "   161/50000: episode: 16, duration: 0.466s, episode steps: 14, steps per second: 30, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.857 [0.000, 1.000], mean observation: -0.069 [-3.010, 1.991], \n",
      "   170/50000: episode: 17, duration: 0.300s, episode steps: 9, steps per second: 30, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.159 [-2.829, 1.718], \n",
      "   179/50000: episode: 18, duration: 0.299s, episode steps: 9, steps per second: 30, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.164 [-2.840, 1.738], \n",
      "   189/50000: episode: 19, duration: 0.333s, episode steps: 10, steps per second: 30, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.146 [-3.121, 1.977], \n",
      "   197/50000: episode: 20, duration: 0.266s, episode steps: 8, steps per second: 30, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.875 [0.000, 1.000], mean observation: -0.137 [-2.230, 1.409], \n",
      "   208/50000: episode: 21, duration: 0.367s, episode steps: 11, steps per second: 30, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.134 [-2.323, 1.373], \n",
      "   218/50000: episode: 22, duration: 0.332s, episode steps: 10, steps per second: 30, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.127 [-2.966, 1.909], \n",
      "   228/50000: episode: 23, duration: 0.333s, episode steps: 10, steps per second: 30, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.131 [-3.013, 1.993], \n",
      "   238/50000: episode: 24, duration: 0.333s, episode steps: 10, steps per second: 30, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.155 [-3.064, 1.951], \n",
      "   247/50000: episode: 25, duration: 0.299s, episode steps: 9, steps per second: 30, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.161 [-2.843, 1.770], \n",
      "   257/50000: episode: 26, duration: 0.333s, episode steps: 10, steps per second: 30, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.152 [-3.108, 1.939], \n",
      "   266/50000: episode: 27, duration: 0.299s, episode steps: 9, steps per second: 30, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.131 [-2.746, 1.803], \n",
      "   277/50000: episode: 28, duration: 0.367s, episode steps: 11, steps per second: 30, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.120 [-2.768, 1.758], \n",
      "   287/50000: episode: 29, duration: 0.334s, episode steps: 10, steps per second: 30, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.132 [-2.982, 1.954], \n",
      "   297/50000: episode: 30, duration: 0.332s, episode steps: 10, steps per second: 30, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.118 [-3.041, 1.977], \n",
      "   307/50000: episode: 31, duration: 0.333s, episode steps: 10, steps per second: 30, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.133 [-3.031, 1.975], \n",
      "   316/50000: episode: 32, duration: 0.298s, episode steps: 9, steps per second: 30, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.145 [-2.843, 1.763], \n",
      "   325/50000: episode: 33, duration: 0.300s, episode steps: 9, steps per second: 30, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.150 [-2.878, 1.762], \n",
      "   335/50000: episode: 34, duration: 0.333s, episode steps: 10, steps per second: 30, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.144 [-3.092, 1.925], \n",
      "   345/50000: episode: 35, duration: 0.332s, episode steps: 10, steps per second: 30, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.134 [-2.549, 1.541], \n",
      "   355/50000: episode: 36, duration: 0.336s, episode steps: 10, steps per second: 30, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.116 [-2.451, 1.588], \n",
      "   365/50000: episode: 37, duration: 0.329s, episode steps: 10, steps per second: 30, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.146 [-3.107, 1.934], \n",
      "   376/50000: episode: 38, duration: 0.366s, episode steps: 11, steps per second: 30, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.137 [-2.824, 1.744], \n",
      "   387/50000: episode: 39, duration: 0.369s, episode steps: 11, steps per second: 30, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.113 [-2.235, 1.410], \n",
      "   397/50000: episode: 40, duration: 0.330s, episode steps: 10, steps per second: 30, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.129 [-2.983, 1.948], \n",
      "   409/50000: episode: 41, duration: 0.401s, episode steps: 12, steps per second: 30, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.095 [-2.739, 1.799], \n",
      "   418/50000: episode: 42, duration: 0.315s, episode steps: 9, steps per second: 29, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.132 [-2.226, 1.339], \n",
      "   427/50000: episode: 43, duration: 0.299s, episode steps: 9, steps per second: 30, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.151 [-2.267, 1.352], \n",
      "   437/50000: episode: 44, duration: 0.331s, episode steps: 10, steps per second: 30, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.148 [-2.590, 1.562], \n",
      "   445/50000: episode: 45, duration: 0.273s, episode steps: 8, steps per second: 29, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.148 [-2.563, 1.552], \n",
      "   455/50000: episode: 46, duration: 0.326s, episode steps: 10, steps per second: 31, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.119 [-2.981, 1.993], \n",
      "   465/50000: episode: 47, duration: 0.333s, episode steps: 10, steps per second: 30, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.142 [-3.053, 1.908], \n",
      "   474/50000: episode: 48, duration: 0.299s, episode steps: 9, steps per second: 30, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.150 [-2.852, 1.755], \n",
      "   486/50000: episode: 49, duration: 0.399s, episode steps: 12, steps per second: 30, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.917 [0.000, 1.000], mean observation: -0.131 [-3.049, 1.909], \n",
      "   494/50000: episode: 50, duration: 0.267s, episode steps: 8, steps per second: 30, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.144 [-2.528, 1.529], \n",
      "   505/50000: episode: 51, duration: 0.365s, episode steps: 11, steps per second: 30, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.114 [-2.822, 1.813], \n",
      "   514/50000: episode: 52, duration: 0.300s, episode steps: 9, steps per second: 30, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.133 [-2.741, 1.792], \n",
      "   522/50000: episode: 53, duration: 0.267s, episode steps: 8, steps per second: 30, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.179 [-2.607, 1.548], \n",
      "   532/50000: episode: 54, duration: 0.334s, episode steps: 10, steps per second: 30, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.129 [-3.020, 1.995], \n",
      "   541/50000: episode: 55, duration: 0.299s, episode steps: 9, steps per second: 30, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.131 [-2.435, 1.561], \n",
      "   549/50000: episode: 56, duration: 0.266s, episode steps: 8, steps per second: 30, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.158 [-2.504, 1.533], \n",
      "   558/50000: episode: 57, duration: 0.299s, episode steps: 9, steps per second: 30, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.155 [-2.816, 1.774], \n",
      "   572/50000: episode: 58, duration: 0.466s, episode steps: 14, steps per second: 30, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.857 [0.000, 1.000], mean observation: -0.090 [-3.010, 1.977], \n",
      "   582/50000: episode: 59, duration: 0.334s, episode steps: 10, steps per second: 30, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.121 [-3.038, 1.995], \n",
      "   591/50000: episode: 60, duration: 0.299s, episode steps: 9, steps per second: 30, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.119 [-2.795, 1.807], \n",
      "   600/50000: episode: 61, duration: 0.300s, episode steps: 9, steps per second: 30, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.152 [-2.849, 1.795], \n",
      "   609/50000: episode: 62, duration: 0.298s, episode steps: 9, steps per second: 30, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.149 [-2.471, 1.516], \n",
      "   620/50000: episode: 63, duration: 0.366s, episode steps: 11, steps per second: 30, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.127 [-2.267, 1.359], \n",
      "   629/50000: episode: 64, duration: 0.299s, episode steps: 9, steps per second: 30, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.122 [-2.777, 1.792], \n",
      "   639/50000: episode: 65, duration: 0.333s, episode steps: 10, steps per second: 30, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.128 [-3.043, 1.968], \n",
      "   649/50000: episode: 66, duration: 0.333s, episode steps: 10, steps per second: 30, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.127 [-2.967, 1.917], \n",
      "   659/50000: episode: 67, duration: 0.332s, episode steps: 10, steps per second: 30, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.125 [-2.236, 1.387], \n",
      "   669/50000: episode: 68, duration: 0.334s, episode steps: 10, steps per second: 30, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.132 [-2.238, 1.357], \n",
      "   680/50000: episode: 69, duration: 0.365s, episode steps: 11, steps per second: 30, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.114 [-2.037, 1.379], \n",
      "   693/50000: episode: 70, duration: 0.433s, episode steps: 13, steps per second: 30, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.079 [-1.758, 1.200], \n",
      "   702/50000: episode: 71, duration: 0.299s, episode steps: 9, steps per second: 30, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.142 [-1.867, 1.168], \n",
      "   712/50000: episode: 72, duration: 0.333s, episode steps: 10, steps per second: 30, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.113 [-1.536, 1.000], \n",
      "   723/50000: episode: 73, duration: 0.367s, episode steps: 11, steps per second: 30, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.108 [-1.568, 0.997], \n",
      "   733/50000: episode: 74, duration: 0.332s, episode steps: 10, steps per second: 30, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.137 [-1.549, 0.942], \n",
      "   744/50000: episode: 75, duration: 0.365s, episode steps: 11, steps per second: 30, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.089 [-1.494, 0.999], \n",
      "   756/50000: episode: 76, duration: 0.400s, episode steps: 12, steps per second: 30, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.111 [-1.520, 0.992], \n",
      "   769/50000: episode: 77, duration: 0.432s, episode steps: 13, steps per second: 30, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.112 [-1.485, 0.961], \n",
      "   781/50000: episode: 78, duration: 0.400s, episode steps: 12, steps per second: 30, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.098 [-1.474, 0.982], \n",
      "   792/50000: episode: 79, duration: 0.365s, episode steps: 11, steps per second: 30, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.092 [-1.485, 1.026], \n",
      "   802/50000: episode: 80, duration: 0.333s, episode steps: 10, steps per second: 30, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.113 [-1.326, 0.824], \n",
      "   817/50000: episode: 81, duration: 0.500s, episode steps: 15, steps per second: 30, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.079 [-1.207, 0.805], \n",
      "   834/50000: episode: 82, duration: 0.566s, episode steps: 17, steps per second: 30, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.088 [-1.208, 0.748], \n",
      "   847/50000: episode: 83, duration: 0.433s, episode steps: 13, steps per second: 30, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.126 [-1.149, 0.587], \n",
      "   866/50000: episode: 84, duration: 0.633s, episode steps: 19, steps per second: 30, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.108 [-0.941, 0.553], \n",
      "   896/50000: episode: 85, duration: 1.000s, episode steps: 30, steps per second: 30, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.267 [0.000, 1.000], mean observation: -0.031 [-2.719, 3.485], \n",
      "   908/50000: episode: 86, duration: 0.399s, episode steps: 12, steps per second: 30, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.130 [-1.535, 2.611], \n",
      "   918/50000: episode: 87, duration: 0.332s, episode steps: 10, steps per second: 30, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.151 [-1.956, 3.077], \n",
      "   947/50000: episode: 88, duration: 0.967s, episode steps: 29, steps per second: 30, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.448 [0.000, 1.000], mean observation: -0.099 [-0.690, 0.399], \n",
      "   964/50000: episode: 89, duration: 0.566s, episode steps: 17, steps per second: 30, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.064 [-0.983, 0.637], \n",
      "   982/50000: episode: 90, duration: 0.599s, episode steps: 18, steps per second: 30, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.090 [-1.158, 0.741], \n",
      "  1002/50000: episode: 91, duration: 0.664s, episode steps: 20, steps per second: 30, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.061 [-1.103, 0.808], \n",
      "  1022/50000: episode: 92, duration: 0.667s, episode steps: 20, steps per second: 30, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.088 [-1.138, 0.778], \n",
      "  1040/50000: episode: 93, duration: 0.600s, episode steps: 18, steps per second: 30, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.120 [-0.978, 0.571], \n",
      "  1101/50000: episode: 94, duration: 2.030s, episode steps: 61, steps per second: 30, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.344 [0.000, 1.000], mean observation: -0.281 [-3.594, 3.282], \n",
      "  1137/50000: episode: 95, duration: 1.199s, episode steps: 36, steps per second: 30, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.099 [-0.711, 0.379], \n",
      "  1148/50000: episode: 96, duration: 0.368s, episode steps: 11, steps per second: 30, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.112 [-1.836, 1.187], \n",
      "  1164/50000: episode: 97, duration: 0.534s, episode steps: 16, steps per second: 30, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.077 [-1.436, 0.992], \n",
      "  1216/50000: episode: 98, duration: 1.732s, episode steps: 52, steps per second: 30, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.346 [0.000, 1.000], mean observation: -0.186 [-3.098, 3.299], \n",
      "  1309/50000: episode: 99, duration: 3.099s, episode steps: 93, steps per second: 30, episode reward: 93.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.387 [0.000, 1.000], mean observation: -0.249 [-3.930, 3.363], \n",
      "  1334/50000: episode: 100, duration: 0.832s, episode steps: 25, steps per second: 30, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: -0.061 [-1.037, 0.625], \n",
      "  1360/50000: episode: 101, duration: 0.866s, episode steps: 26, steps per second: 30, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.423 [0.000, 1.000], mean observation: -0.134 [-0.941, 0.570], \n",
      "  1441/50000: episode: 102, duration: 2.702s, episode steps: 81, steps per second: 30, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.110 [-0.894, 0.476], \n",
      "  1499/50000: episode: 103, duration: 1.931s, episode steps: 58, steps per second: 30, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.065 [-0.774, 0.417], \n",
      "  1509/50000: episode: 104, duration: 0.334s, episode steps: 10, steps per second: 30, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.101 [-2.439, 1.608], \n",
      "  1518/50000: episode: 105, duration: 0.298s, episode steps: 9, steps per second: 30, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.167 [-2.827, 1.735], \n",
      "  1526/50000: episode: 106, duration: 0.267s, episode steps: 8, steps per second: 30, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.143 [-2.537, 1.581], \n",
      "  1539/50000: episode: 107, duration: 0.433s, episode steps: 13, steps per second: 30, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.078 [-1.724, 1.210], \n",
      "  1551/50000: episode: 108, duration: 0.400s, episode steps: 12, steps per second: 30, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.106 [-1.755, 1.166], \n",
      "  1568/50000: episode: 109, duration: 0.565s, episode steps: 17, steps per second: 30, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.102 [-1.175, 0.755], \n",
      "  1612/50000: episode: 110, duration: 1.467s, episode steps: 44, steps per second: 30, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.386 [0.000, 1.000], mean observation: -0.057 [-1.914, 2.337], \n",
      "  1623/50000: episode: 111, duration: 0.366s, episode steps: 11, steps per second: 30, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.120 [-2.182, 3.293], \n",
      "  1633/50000: episode: 112, duration: 0.333s, episode steps: 10, steps per second: 30, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.126 [-1.959, 3.045], \n",
      "  1644/50000: episode: 113, duration: 0.366s, episode steps: 11, steps per second: 30, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.128 [-1.790, 2.878], \n",
      "  1654/50000: episode: 114, duration: 0.332s, episode steps: 10, steps per second: 30, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.118 [-1.976, 3.028], \n",
      "  1664/50000: episode: 115, duration: 0.333s, episode steps: 10, steps per second: 30, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.136 [-1.980, 3.050], \n",
      "  1674/50000: episode: 116, duration: 0.332s, episode steps: 10, steps per second: 30, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.180 [-1.912, 3.135], \n",
      "  1684/50000: episode: 117, duration: 0.333s, episode steps: 10, steps per second: 30, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.140 [-1.725, 2.713], \n",
      "  1694/50000: episode: 118, duration: 0.333s, episode steps: 10, steps per second: 30, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.137 [-1.543, 2.454], \n",
      "  1703/50000: episode: 119, duration: 0.299s, episode steps: 9, steps per second: 30, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.145 [-1.793, 2.859], \n",
      "  1713/50000: episode: 120, duration: 0.333s, episode steps: 10, steps per second: 30, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.138 [-1.985, 3.067], \n",
      "  1863/50000: episode: 121, duration: 5.000s, episode steps: 150, steps per second: 30, episode reward: 150.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.053 [-1.273, 1.171], \n",
      "  1888/50000: episode: 122, duration: 0.832s, episode steps: 25, steps per second: 30, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: -0.070 [-1.114, 0.809], \n",
      "  1901/50000: episode: 123, duration: 0.434s, episode steps: 13, steps per second: 30, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.099 [-1.197, 0.828], \n",
      "  1917/50000: episode: 124, duration: 0.532s, episode steps: 16, steps per second: 30, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: -0.079 [-1.166, 0.785], \n",
      "  1933/50000: episode: 125, duration: 0.532s, episode steps: 16, steps per second: 30, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.115 [-0.960, 0.582], \n",
      "  1944/50000: episode: 126, duration: 0.366s, episode steps: 11, steps per second: 30, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.122 [-1.507, 0.955], \n",
      "  1958/50000: episode: 127, duration: 0.467s, episode steps: 14, steps per second: 30, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.112 [-1.248, 0.737], \n",
      "  1977/50000: episode: 128, duration: 0.631s, episode steps: 19, steps per second: 30, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.065 [-1.437, 0.993], \n",
      "  2047/50000: episode: 129, duration: 2.333s, episode steps: 70, steps per second: 30, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.310 [-1.501, 0.560], \n",
      "  2123/50000: episode: 130, duration: 2.533s, episode steps: 76, steps per second: 30, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.461 [0.000, 1.000], mean observation: -0.301 [-1.240, 0.801], \n",
      "  2138/50000: episode: 131, duration: 0.498s, episode steps: 15, steps per second: 30, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.115 [-1.002, 0.557], \n",
      "  2208/50000: episode: 132, duration: 2.333s, episode steps: 70, steps per second: 30, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: -0.327 [-1.848, 0.761], \n",
      "  2223/50000: episode: 133, duration: 0.499s, episode steps: 15, steps per second: 30, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.102 [-1.198, 0.768], \n",
      "  2237/50000: episode: 134, duration: 0.467s, episode steps: 14, steps per second: 30, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.080 [-1.412, 0.967], \n",
      "  2247/50000: episode: 135, duration: 0.332s, episode steps: 10, steps per second: 30, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.102 [-1.808, 1.192], \n",
      "  2259/50000: episode: 136, duration: 0.400s, episode steps: 12, steps per second: 30, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.109 [-1.907, 1.165], \n",
      "  2268/50000: episode: 137, duration: 0.299s, episode steps: 9, steps per second: 30, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.162 [-2.209, 1.377], \n",
      "  2279/50000: episode: 138, duration: 0.367s, episode steps: 11, steps per second: 30, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.112 [-2.334, 1.609], \n",
      "  2290/50000: episode: 139, duration: 0.366s, episode steps: 11, steps per second: 30, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.123 [-2.300, 1.543], \n",
      "  2303/50000: episode: 140, duration: 0.433s, episode steps: 13, steps per second: 30, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.072 [-2.253, 1.578], \n",
      "  2314/50000: episode: 141, duration: 0.366s, episode steps: 11, steps per second: 30, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.115 [-1.853, 1.196], \n",
      "  2323/50000: episode: 142, duration: 0.299s, episode steps: 9, steps per second: 30, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.116 [-1.593, 1.024], \n",
      "  2338/50000: episode: 143, duration: 0.499s, episode steps: 15, steps per second: 30, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.092 [-1.480, 0.963], \n",
      "  2355/50000: episode: 144, duration: 0.567s, episode steps: 17, steps per second: 30, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.107 [-1.166, 0.750], \n",
      "  2478/50000: episode: 145, duration: 4.099s, episode steps: 123, steps per second: 30, episode reward: 123.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.431 [0.000, 1.000], mean observation: -0.347 [-3.271, 3.360], \n",
      "  2538/50000: episode: 146, duration: 1.999s, episode steps: 60, steps per second: 30, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.166 [-0.514, 1.016], \n",
      "  2611/50000: episode: 147, duration: 2.432s, episode steps: 73, steps per second: 30, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.534 [0.000, 1.000], mean observation: 0.191 [-0.625, 0.953], \n",
      "  2677/50000: episode: 148, duration: 2.200s, episode steps: 66, steps per second: 30, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.530 [0.000, 1.000], mean observation: 0.164 [-0.402, 0.770], \n",
      "  2824/50000: episode: 149, duration: 4.898s, episode steps: 147, steps per second: 30, episode reward: 147.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.551 [0.000, 1.000], mean observation: 0.349 [-0.667, 2.769], \n",
      "  2833/50000: episode: 150, duration: 0.300s, episode steps: 9, steps per second: 30, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.155 [-1.738, 2.817], \n",
      "  2843/50000: episode: 151, duration: 0.332s, episode steps: 10, steps per second: 30, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.146 [-1.945, 3.051], \n",
      "  2853/50000: episode: 152, duration: 0.333s, episode steps: 10, steps per second: 30, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.175 [-1.938, 3.143], \n",
      "  2864/50000: episode: 153, duration: 0.366s, episode steps: 11, steps per second: 30, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.146 [-1.737, 2.799], \n",
      "  2874/50000: episode: 154, duration: 0.333s, episode steps: 10, steps per second: 30, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.140 [-1.931, 3.026], \n",
      "  2883/50000: episode: 155, duration: 0.299s, episode steps: 9, steps per second: 30, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.120 [-1.201, 1.920], \n",
      "  2945/50000: episode: 156, duration: 2.067s, episode steps: 62, steps per second: 30, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.093 [-0.756, 1.177], \n",
      "  2992/50000: episode: 157, duration: 1.564s, episode steps: 47, steps per second: 30, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.426 [0.000, 1.000], mean observation: -0.244 [-1.309, 0.359], \n",
      "  3035/50000: episode: 158, duration: 1.434s, episode steps: 43, steps per second: 30, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.419 [0.000, 1.000], mean observation: -0.203 [-1.309, 0.556], \n",
      "  3065/50000: episode: 159, duration: 0.999s, episode steps: 30, steps per second: 30, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.433 [0.000, 1.000], mean observation: -0.120 [-1.118, 0.743], \n",
      "  3079/50000: episode: 160, duration: 0.464s, episode steps: 14, steps per second: 30, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.114 [-0.998, 0.542], \n",
      "  3093/50000: episode: 161, duration: 0.467s, episode steps: 14, steps per second: 30, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.079 [-1.438, 1.023], \n",
      "  3103/50000: episode: 162, duration: 0.332s, episode steps: 10, steps per second: 30, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.135 [-1.582, 1.009], \n",
      "  3114/50000: episode: 163, duration: 0.367s, episode steps: 11, steps per second: 30, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.114 [-1.298, 0.787], \n",
      "  3137/50000: episode: 164, duration: 0.767s, episode steps: 23, steps per second: 30, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.435 [0.000, 1.000], mean observation: -0.097 [-1.184, 0.786], \n",
      "  3202/50000: episode: 165, duration: 2.166s, episode steps: 65, steps per second: 30, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.446 [0.000, 1.000], mean observation: -0.289 [-1.515, 0.543], \n",
      "  3220/50000: episode: 166, duration: 0.598s, episode steps: 18, steps per second: 30, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.072 [-1.174, 0.824], \n",
      "  3235/50000: episode: 167, duration: 0.500s, episode steps: 15, steps per second: 30, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.133 [-1.004, 0.559], \n",
      "  3291/50000: episode: 168, duration: 1.866s, episode steps: 56, steps per second: 30, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: -0.284 [-1.500, 0.550], \n",
      "  3355/50000: episode: 169, duration: 2.132s, episode steps: 64, steps per second: 30, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: -0.280 [-1.416, 0.626], \n",
      "  3375/50000: episode: 170, duration: 0.667s, episode steps: 20, steps per second: 30, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.071 [-1.105, 0.761], \n",
      "  3399/50000: episode: 171, duration: 0.801s, episode steps: 24, steps per second: 30, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: -0.101 [-0.960, 0.630], \n",
      "  3418/50000: episode: 172, duration: 0.630s, episode steps: 19, steps per second: 30, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.112 [-1.181, 0.752], \n",
      "  3438/50000: episode: 173, duration: 0.667s, episode steps: 20, steps per second: 30, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: -0.090 [-1.269, 0.768], \n",
      "  3486/50000: episode: 174, duration: 1.599s, episode steps: 48, steps per second: 30, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: -0.216 [-1.439, 0.607], \n",
      "  3504/50000: episode: 175, duration: 0.600s, episode steps: 18, steps per second: 30, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.062 [-1.411, 1.007], \n",
      "  3557/50000: episode: 176, duration: 1.765s, episode steps: 53, steps per second: 30, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.415 [0.000, 1.000], mean observation: -0.227 [-1.616, 0.807], \n",
      "  3626/50000: episode: 177, duration: 2.300s, episode steps: 69, steps per second: 30, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.449 [0.000, 1.000], mean observation: -0.249 [-1.425, 0.807], \n",
      "  3638/50000: episode: 178, duration: 0.398s, episode steps: 12, steps per second: 30, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.094 [-1.504, 0.990], \n",
      "  3648/50000: episode: 179, duration: 0.334s, episode steps: 10, steps per second: 30, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.112 [-1.299, 0.772], \n",
      "  3661/50000: episode: 180, duration: 0.432s, episode steps: 13, steps per second: 30, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.085 [-1.700, 1.204], \n",
      "  3674/50000: episode: 181, duration: 0.433s, episode steps: 13, steps per second: 30, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.113 [-1.509, 0.953], \n",
      "  3688/50000: episode: 182, duration: 0.466s, episode steps: 14, steps per second: 30, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.095 [-1.482, 1.013], \n",
      "  3703/50000: episode: 183, duration: 0.499s, episode steps: 15, steps per second: 30, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.068 [-1.385, 0.983], \n",
      "  3725/50000: episode: 184, duration: 0.733s, episode steps: 22, steps per second: 30, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.092 [-1.194, 0.801], \n",
      "  3802/50000: episode: 185, duration: 2.566s, episode steps: 77, steps per second: 30, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.442 [0.000, 1.000], mean observation: -0.280 [-1.599, 0.807], \n",
      "  3913/50000: episode: 186, duration: 3.698s, episode steps: 111, steps per second: 30, episode reward: 111.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.214 [-1.292, 0.438], \n",
      "  4136/50000: episode: 187, duration: 7.432s, episode steps: 223, steps per second: 30, episode reward: 223.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.070 [-1.235, 0.797], \n",
      "  4151/50000: episode: 188, duration: 0.499s, episode steps: 15, steps per second: 30, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.111 [-0.952, 1.793], \n",
      "  4161/50000: episode: 189, duration: 0.334s, episode steps: 10, steps per second: 30, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.135 [-1.575, 2.564], \n",
      "  4172/50000: episode: 190, duration: 0.365s, episode steps: 11, steps per second: 30, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.128 [-1.753, 2.841], \n",
      "  4205/50000: episode: 191, duration: 1.101s, episode steps: 33, steps per second: 30, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.606 [0.000, 1.000], mean observation: 0.106 [-1.620, 1.713], \n",
      "  4265/50000: episode: 192, duration: 1.998s, episode steps: 60, steps per second: 30, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.433 [0.000, 1.000], mean observation: -0.283 [-1.592, 0.820], \n",
      "  4280/50000: episode: 193, duration: 0.499s, episode steps: 15, steps per second: 30, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.069 [-1.400, 0.969], \n",
      "  4291/50000: episode: 194, duration: 0.366s, episode steps: 11, steps per second: 30, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.133 [-2.093, 1.335], \n",
      "  4302/50000: episode: 195, duration: 0.367s, episode steps: 11, steps per second: 30, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.123 [-2.172, 1.340], \n",
      "  4315/50000: episode: 196, duration: 0.433s, episode steps: 13, steps per second: 30, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.080 [-2.070, 1.410], \n",
      "  4327/50000: episode: 197, duration: 0.399s, episode steps: 12, steps per second: 30, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.119 [-2.017, 1.338], \n",
      "  4339/50000: episode: 198, duration: 0.399s, episode steps: 12, steps per second: 30, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.085 [-1.557, 1.024], \n",
      "  4351/50000: episode: 199, duration: 0.399s, episode steps: 12, steps per second: 30, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.121 [-1.323, 0.756], \n",
      "  4365/50000: episode: 200, duration: 0.466s, episode steps: 14, steps per second: 30, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.099 [-1.266, 0.788], \n",
      "  4383/50000: episode: 201, duration: 0.598s, episode steps: 18, steps per second: 30, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: -0.091 [-1.228, 0.829], \n",
      "  4408/50000: episode: 202, duration: 0.834s, episode steps: 25, steps per second: 30, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: -0.073 [-1.163, 0.827], \n",
      "  4434/50000: episode: 203, duration: 0.865s, episode steps: 26, steps per second: 30, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.106 [-1.155, 0.751], \n",
      "  4519/50000: episode: 204, duration: 2.835s, episode steps: 85, steps per second: 30, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.447 [0.000, 1.000], mean observation: -0.309 [-1.610, 0.618], \n",
      "  4615/50000: episode: 205, duration: 3.198s, episode steps: 96, steps per second: 30, episode reward: 96.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.299 [-1.502, 0.440], \n",
      "  4693/50000: episode: 206, duration: 2.598s, episode steps: 78, steps per second: 30, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.551 [0.000, 1.000], mean observation: 0.312 [-0.557, 1.665], \n",
      "  4703/50000: episode: 207, duration: 0.332s, episode steps: 10, steps per second: 30, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.152 [-1.988, 3.104], \n",
      "  4712/50000: episode: 208, duration: 0.300s, episode steps: 9, steps per second: 30, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.149 [-1.725, 2.790], \n",
      "  4721/50000: episode: 209, duration: 0.299s, episode steps: 9, steps per second: 30, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.156 [-1.806, 2.887], \n",
      "  4729/50000: episode: 210, duration: 0.266s, episode steps: 8, steps per second: 30, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.125 [0.000, 1.000], mean observation: 0.145 [-1.203, 1.981], \n",
      "  4740/50000: episode: 211, duration: 0.365s, episode steps: 11, steps per second: 30, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.116 [-1.810, 2.846], \n",
      "  4749/50000: episode: 212, duration: 0.300s, episode steps: 9, steps per second: 30, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.161 [-1.716, 2.783], \n",
      "  4886/50000: episode: 213, duration: 4.567s, episode steps: 137, steps per second: 30, episode reward: 137.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.540 [0.000, 1.000], mean observation: 0.440 [-0.940, 2.426], \n",
      "  4985/50000: episode: 214, duration: 3.299s, episode steps: 99, steps per second: 30, episode reward: 99.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.166 [-1.236, 0.426], \n",
      "  5077/50000: episode: 215, duration: 3.065s, episode steps: 92, steps per second: 30, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.159 [-1.115, 0.472], \n",
      "  5146/50000: episode: 216, duration: 2.300s, episode steps: 69, steps per second: 30, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.449 [0.000, 1.000], mean observation: -0.195 [-1.230, 0.644], \n",
      "  5221/50000: episode: 217, duration: 2.498s, episode steps: 75, steps per second: 30, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.453 [0.000, 1.000], mean observation: -0.204 [-1.326, 0.543], \n",
      "  5286/50000: episode: 218, duration: 2.165s, episode steps: 65, steps per second: 30, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.446 [0.000, 1.000], mean observation: -0.215 [-1.256, 0.405], \n",
      "  5345/50000: episode: 219, duration: 1.966s, episode steps: 59, steps per second: 30, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.441 [0.000, 1.000], mean observation: -0.286 [-1.627, 0.594], \n",
      "  5402/50000: episode: 220, duration: 1.900s, episode steps: 57, steps per second: 30, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.439 [0.000, 1.000], mean observation: -0.286 [-1.471, 0.597], \n",
      "  5415/50000: episode: 221, duration: 0.432s, episode steps: 13, steps per second: 30, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.070 [-1.676, 1.187], \n",
      "  5427/50000: episode: 222, duration: 0.400s, episode steps: 12, steps per second: 30, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.095 [-1.222, 0.793], \n",
      "  5443/50000: episode: 223, duration: 0.532s, episode steps: 16, steps per second: 30, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.092 [-1.166, 0.740], \n",
      "  5521/50000: episode: 224, duration: 2.599s, episode steps: 78, steps per second: 30, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.436 [0.000, 1.000], mean observation: -0.373 [-1.857, 0.734], \n",
      "  5667/50000: episode: 225, duration: 4.865s, episode steps: 146, steps per second: 30, episode reward: 146.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.548 [0.000, 1.000], mean observation: 0.356 [-0.552, 2.583], \n",
      "  5676/50000: episode: 226, duration: 0.300s, episode steps: 9, steps per second: 30, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.146 [-1.789, 2.824], \n",
      "  5688/50000: episode: 227, duration: 0.399s, episode steps: 12, steps per second: 30, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.083 [0.000, 1.000], mean observation: 0.128 [-1.905, 3.040], \n",
      "  5700/50000: episode: 228, duration: 0.399s, episode steps: 12, steps per second: 30, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.083 [0.000, 1.000], mean observation: 0.124 [-1.998, 3.102], \n",
      "  5796/50000: episode: 229, duration: 3.199s, episode steps: 96, steps per second: 30, episode reward: 96.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.197 [-1.493, 0.537], \n",
      "  5874/50000: episode: 230, duration: 2.599s, episode steps: 78, steps per second: 30, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.449 [0.000, 1.000], mean observation: -0.183 [-1.447, 0.707], \n",
      "  5887/50000: episode: 231, duration: 0.434s, episode steps: 13, steps per second: 30, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.088 [-1.180, 0.635], \n",
      "  5905/50000: episode: 232, duration: 0.598s, episode steps: 18, steps per second: 30, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.062 [-1.415, 1.000], \n",
      "  5917/50000: episode: 233, duration: 0.399s, episode steps: 12, steps per second: 30, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.109 [-1.229, 0.817], \n",
      "  5934/50000: episode: 234, duration: 0.566s, episode steps: 17, steps per second: 30, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.087 [-1.162, 0.799], \n",
      "  5950/50000: episode: 235, duration: 0.533s, episode steps: 16, steps per second: 30, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.081 [-1.178, 0.830], \n",
      "  5975/50000: episode: 236, duration: 0.834s, episode steps: 25, steps per second: 30, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: -0.105 [-0.949, 0.625], \n",
      "  6051/50000: episode: 237, duration: 2.532s, episode steps: 76, steps per second: 30, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.447 [0.000, 1.000], mean observation: -0.285 [-1.485, 0.566], \n",
      "  6131/50000: episode: 238, duration: 2.665s, episode steps: 80, steps per second: 30, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: -0.328 [-1.840, 0.375], \n",
      "  6296/50000: episode: 239, duration: 5.500s, episode steps: 165, steps per second: 30, episode reward: 165.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.302 [-0.404, 2.017], \n",
      "  6317/50000: episode: 240, duration: 0.699s, episode steps: 21, steps per second: 30, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.068 [-0.916, 0.601], \n",
      "  6334/50000: episode: 241, duration: 0.565s, episode steps: 17, steps per second: 30, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.105 [-1.198, 0.778], \n",
      "  6348/50000: episode: 242, duration: 0.467s, episode steps: 14, steps per second: 30, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.112 [-1.144, 0.571], \n",
      "  6371/50000: episode: 243, duration: 0.766s, episode steps: 23, steps per second: 30, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.435 [0.000, 1.000], mean observation: -0.128 [-0.933, 0.583], \n",
      "  6452/50000: episode: 244, duration: 2.699s, episode steps: 81, steps per second: 30, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: -0.366 [-1.647, 0.431], \n",
      "  6569/50000: episode: 245, duration: 3.899s, episode steps: 117, steps per second: 30, episode reward: 117.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.171 [-0.398, 1.630], \n",
      "  6579/50000: episode: 246, duration: 0.333s, episode steps: 10, steps per second: 30, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.142 [-1.795, 2.738], \n",
      "  6591/50000: episode: 247, duration: 0.399s, episode steps: 12, steps per second: 30, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.083 [0.000, 1.000], mean observation: 0.148 [-1.939, 3.114], \n",
      "  6602/50000: episode: 248, duration: 0.366s, episode steps: 11, steps per second: 30, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.098 [-0.962, 1.566], \n",
      "  6703/50000: episode: 249, duration: 3.366s, episode steps: 101, steps per second: 30, episode reward: 101.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.554 [0.000, 1.000], mean observation: 0.450 [-0.800, 2.013], \n",
      "  6907/50000: episode: 250, duration: 6.799s, episode steps: 204, steps per second: 30, episode reward: 204.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.333 [-0.490, 2.362], \n",
      "  6925/50000: episode: 251, duration: 0.600s, episode steps: 18, steps per second: 30, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.110 [-0.972, 0.562], \n",
      "  6997/50000: episode: 252, duration: 2.397s, episode steps: 72, steps per second: 30, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.431 [0.000, 1.000], mean observation: -0.308 [-1.862, 0.731], \n",
      "  7079/50000: episode: 253, duration: 2.732s, episode steps: 82, steps per second: 30, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.252 [-1.294, 0.561], \n",
      "  7158/50000: episode: 254, duration: 2.635s, episode steps: 79, steps per second: 30, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.443 [0.000, 1.000], mean observation: -0.306 [-1.886, 0.642], \n",
      "  7317/50000: episode: 255, duration: 5.300s, episode steps: 159, steps per second: 30, episode reward: 159.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.149 [-1.277, 0.573], \n",
      "  7480/50000: episode: 256, duration: 5.414s, episode steps: 163, steps per second: 30, episode reward: 163.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.126 [-1.269, 0.584], \n",
      "  7498/50000: episode: 257, duration: 0.599s, episode steps: 18, steps per second: 30, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: -0.128 [-0.956, 0.551], \n",
      "  7530/50000: episode: 258, duration: 1.067s, episode steps: 32, steps per second: 30, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: -0.122 [-1.169, 0.760], \n",
      "  7693/50000: episode: 259, duration: 5.434s, episode steps: 163, steps per second: 30, episode reward: 163.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.540 [0.000, 1.000], mean observation: 0.342 [-0.568, 2.381], \n",
      "  7703/50000: episode: 260, duration: 0.331s, episode steps: 10, steps per second: 30, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.156 [-1.520, 2.611], \n",
      "  7714/50000: episode: 261, duration: 0.366s, episode steps: 11, steps per second: 30, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.092 [-1.219, 1.801], \n",
      "  7837/50000: episode: 262, duration: 4.099s, episode steps: 123, steps per second: 30, episode reward: 123.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.512 [-0.976, 2.410], \n",
      "  7940/50000: episode: 263, duration: 3.432s, episode steps: 103, steps per second: 30, episode reward: 103.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.188 [-1.315, 0.636], \n",
      "  8015/50000: episode: 264, duration: 2.499s, episode steps: 75, steps per second: 30, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.453 [0.000, 1.000], mean observation: -0.228 [-1.292, 0.369], \n",
      "  8084/50000: episode: 265, duration: 2.299s, episode steps: 69, steps per second: 30, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.420 [0.000, 1.000], mean observation: -0.323 [-1.972, 0.626], \n",
      "  8168/50000: episode: 266, duration: 2.799s, episode steps: 84, steps per second: 30, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.197 [-1.223, 0.397], \n",
      "  8276/50000: episode: 267, duration: 3.598s, episode steps: 108, steps per second: 30, episode reward: 108.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.229 [-1.475, 0.467], \n",
      "  8395/50000: episode: 268, duration: 3.965s, episode steps: 119, steps per second: 30, episode reward: 119.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.185 [-1.287, 0.575], \n",
      "  8424/50000: episode: 269, duration: 0.967s, episode steps: 29, steps per second: 30, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.448 [0.000, 1.000], mean observation: -0.138 [-0.956, 0.560], \n",
      "  8552/50000: episode: 270, duration: 4.265s, episode steps: 128, steps per second: 30, episode reward: 128.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.231 [-1.492, 0.523], \n",
      "  8704/50000: episode: 271, duration: 5.065s, episode steps: 152, steps per second: 30, episode reward: 152.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.201 [-1.451, 0.537], \n",
      "  8827/50000: episode: 272, duration: 4.098s, episode steps: 123, steps per second: 30, episode reward: 123.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.280 [-1.798, 0.610], \n",
      "  8978/50000: episode: 273, duration: 5.034s, episode steps: 151, steps per second: 30, episode reward: 151.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.543 [0.000, 1.000], mean observation: 0.394 [-0.460, 2.553], \n",
      "  8994/50000: episode: 274, duration: 0.535s, episode steps: 16, steps per second: 30, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.104 [-0.942, 1.724], \n",
      "  9080/50000: episode: 275, duration: 2.864s, episode steps: 86, steps per second: 30, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.535 [0.000, 1.000], mean observation: 0.183 [-0.612, 1.107], \n",
      "  9171/50000: episode: 276, duration: 3.033s, episode steps: 91, steps per second: 30, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.288 [-0.411, 1.623], \n",
      "  9246/50000: episode: 277, duration: 2.498s, episode steps: 75, steps per second: 30, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.427 [0.000, 1.000], mean observation: -0.325 [-2.051, 0.539], \n",
      "  9338/50000: episode: 278, duration: 3.067s, episode steps: 92, steps per second: 30, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.446 [0.000, 1.000], mean observation: -0.374 [-1.833, 0.373], \n",
      "  9551/50000: episode: 279, duration: 7.097s, episode steps: 213, steps per second: 30, episode reward: 213.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.099 [-0.420, 1.317], \n",
      "  9625/50000: episode: 280, duration: 2.466s, episode steps: 74, steps per second: 30, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.419 [0.000, 1.000], mean observation: -0.367 [-2.175, 0.606], \n",
      "  9720/50000: episode: 281, duration: 3.165s, episode steps: 95, steps per second: 30, episode reward: 95.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.442 [0.000, 1.000], mean observation: -0.343 [-1.956, 0.431], \n",
      "  9906/50000: episode: 282, duration: 6.199s, episode steps: 186, steps per second: 30, episode reward: 186.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.221 [-1.851, 0.756], \n",
      " 10049/50000: episode: 283, duration: 4.764s, episode steps: 143, steps per second: 30, episode reward: 143.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.282 [-1.995, 0.585], \n",
      " 10193/50000: episode: 284, duration: 4.799s, episode steps: 144, steps per second: 30, episode reward: 144.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.535 [0.000, 1.000], mean observation: 0.155 [-0.543, 1.840], \n",
      " 10291/50000: episode: 285, duration: 3.266s, episode steps: 98, steps per second: 30, episode reward: 98.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.439 [0.000, 1.000], mean observation: -0.429 [-2.227, 0.358], \n",
      " 10362/50000: episode: 286, duration: 2.365s, episode steps: 71, steps per second: 30, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.535 [0.000, 1.000], mean observation: 0.091 [-0.281, 0.877], \n",
      " 10524/50000: episode: 287, duration: 5.397s, episode steps: 162, steps per second: 30, episode reward: 162.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.047 [-0.750, 1.312], \n",
      " 10538/50000: episode: 288, duration: 0.466s, episode steps: 14, steps per second: 30, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.092 [-0.773, 1.378], \n",
      " 10547/50000: episode: 289, duration: 0.299s, episode steps: 9, steps per second: 30, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.130 [-1.784, 2.780], \n",
      " 10556/50000: episode: 290, duration: 0.283s, episode steps: 9, steps per second: 32, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.115 [-1.419, 2.250], \n",
      " 10575/50000: episode: 291, duration: 0.632s, episode steps: 19, steps per second: 30, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.066 [-0.825, 1.143], \n",
      " 10663/50000: episode: 292, duration: 2.933s, episode steps: 88, steps per second: 30, episode reward: 88.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.206 [-1.500, 0.579], \n",
      " 10742/50000: episode: 293, duration: 2.632s, episode steps: 79, steps per second: 30, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.443 [0.000, 1.000], mean observation: -0.233 [-1.671, 0.356], \n",
      " 10774/50000: episode: 294, duration: 1.066s, episode steps: 32, steps per second: 30, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: -0.129 [-0.683, 0.439], \n",
      " 10845/50000: episode: 295, duration: 2.365s, episode steps: 71, steps per second: 30, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.451 [0.000, 1.000], mean observation: -0.197 [-1.234, 0.437], \n",
      " 10920/50000: episode: 296, duration: 2.499s, episode steps: 75, steps per second: 30, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: -0.193 [-1.624, 0.404], \n",
      " 10997/50000: episode: 297, duration: 2.567s, episode steps: 77, steps per second: 30, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: -0.279 [-1.993, 0.412], \n",
      " 11043/50000: episode: 298, duration: 1.532s, episode steps: 46, steps per second: 30, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.435 [0.000, 1.000], mean observation: -0.159 [-1.129, 0.358], \n",
      " 11106/50000: episode: 299, duration: 2.099s, episode steps: 63, steps per second: 30, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: -0.168 [-1.248, 0.414], \n",
      " 11158/50000: episode: 300, duration: 1.736s, episode steps: 52, steps per second: 30, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.442 [0.000, 1.000], mean observation: -0.157 [-1.066, 0.417], \n",
      " 11235/50000: episode: 301, duration: 2.563s, episode steps: 77, steps per second: 30, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.217 [-1.449, 0.614], \n",
      " 11381/50000: episode: 302, duration: 4.866s, episode steps: 146, steps per second: 30, episode reward: 146.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.119 [-1.450, 0.401], \n",
      " 11469/50000: episode: 303, duration: 2.931s, episode steps: 88, steps per second: 30, episode reward: 88.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.209 [-1.494, 0.246], \n",
      " 11536/50000: episode: 304, duration: 2.233s, episode steps: 67, steps per second: 30, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.448 [0.000, 1.000], mean observation: -0.199 [-1.443, 0.407], \n",
      " 11594/50000: episode: 305, duration: 1.933s, episode steps: 58, steps per second: 30, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.448 [0.000, 1.000], mean observation: -0.204 [-1.285, 0.385], \n",
      " 11702/50000: episode: 306, duration: 3.599s, episode steps: 108, steps per second: 30, episode reward: 108.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.454 [0.000, 1.000], mean observation: -0.205 [-1.776, 0.468], \n",
      " 11779/50000: episode: 307, duration: 2.565s, episode steps: 77, steps per second: 30, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.416 [0.000, 1.000], mean observation: -0.424 [-2.401, 0.355], \n",
      " 11864/50000: episode: 308, duration: 2.832s, episode steps: 85, steps per second: 30, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.447 [0.000, 1.000], mean observation: -0.225 [-1.637, 0.390], \n",
      " 11937/50000: episode: 309, duration: 2.432s, episode steps: 73, steps per second: 30, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: -0.259 [-1.675, 0.357], \n",
      " 12006/50000: episode: 310, duration: 2.299s, episode steps: 69, steps per second: 30, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.449 [0.000, 1.000], mean observation: -0.190 [-1.295, 0.369], \n",
      " 12059/50000: episode: 311, duration: 1.766s, episode steps: 53, steps per second: 30, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.434 [0.000, 1.000], mean observation: -0.202 [-1.275, 0.202], \n",
      " 12157/50000: episode: 312, duration: 3.266s, episode steps: 98, steps per second: 30, episode reward: 98.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.449 [0.000, 1.000], mean observation: -0.224 [-1.788, 0.428], \n",
      " 12232/50000: episode: 313, duration: 2.500s, episode steps: 75, steps per second: 30, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.453 [0.000, 1.000], mean observation: -0.183 [-1.306, 0.454], \n",
      " 12371/50000: episode: 314, duration: 4.630s, episode steps: 139, steps per second: 30, episode reward: 139.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.116 [-1.595, 0.482], \n",
      " 12466/50000: episode: 315, duration: 3.166s, episode steps: 95, steps per second: 30, episode reward: 95.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.432 [0.000, 1.000], mean observation: -0.302 [-2.334, 0.426], \n",
      " 12518/50000: episode: 316, duration: 1.732s, episode steps: 52, steps per second: 30, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.170 [-0.883, 0.433], \n",
      " 12799/50000: episode: 317, duration: 9.365s, episode steps: 281, steps per second: 30, episode reward: 281.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.013 [-1.253, 0.555], \n",
      " 12886/50000: episode: 318, duration: 2.899s, episode steps: 87, steps per second: 30, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.460 [0.000, 1.000], mean observation: -0.220 [-1.479, 0.369], \n",
      " 12964/50000: episode: 319, duration: 2.600s, episode steps: 78, steps per second: 30, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.436 [0.000, 1.000], mean observation: -0.324 [-1.828, 0.445], \n",
      " 13061/50000: episode: 320, duration: 3.232s, episode steps: 97, steps per second: 30, episode reward: 97.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.162 [-1.421, 0.440], \n",
      " 13134/50000: episode: 321, duration: 2.432s, episode steps: 73, steps per second: 30, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: -0.264 [-1.662, 0.265], \n",
      " 13569/50000: episode: 322, duration: 14.498s, episode steps: 435, steps per second: 30, episode reward: 435.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: 0.310 [-0.562, 2.413], \n",
      " 13643/50000: episode: 323, duration: 2.465s, episode steps: 74, steps per second: 30, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: 0.039 [-0.426, 0.693], \n",
      " 13784/50000: episode: 324, duration: 4.699s, episode steps: 141, steps per second: 30, episode reward: 141.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.079 [-0.549, 1.305], \n",
      " 13890/50000: episode: 325, duration: 3.532s, episode steps: 106, steps per second: 30, episode reward: 106.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.434 [0.000, 1.000], mean observation: -0.511 [-2.689, 0.504], \n",
      " 14098/50000: episode: 326, duration: 6.931s, episode steps: 208, steps per second: 30, episode reward: 208.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.345 [-2.420, 0.534], \n",
      " 14164/50000: episode: 327, duration: 2.200s, episode steps: 66, steps per second: 30, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.112 [-0.420, 1.058], \n",
      " 14186/50000: episode: 328, duration: 0.733s, episode steps: 22, steps per second: 30, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.091 [-0.409, 0.837], \n",
      " 14206/50000: episode: 329, duration: 0.665s, episode steps: 20, steps per second: 30, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.107 [-0.545, 0.921], \n",
      " 14274/50000: episode: 330, duration: 2.266s, episode steps: 68, steps per second: 30, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.559 [0.000, 1.000], mean observation: 0.254 [-0.413, 1.432], \n",
      " 14404/50000: episode: 331, duration: 4.330s, episode steps: 130, steps per second: 30, episode reward: 130.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.446 [0.000, 1.000], mean observation: -0.429 [-2.538, 0.392], \n",
      " 14513/50000: episode: 332, duration: 3.634s, episode steps: 109, steps per second: 30, episode reward: 109.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: -0.464 [-2.396, 0.480], \n",
      " 14582/50000: episode: 333, duration: 2.299s, episode steps: 69, steps per second: 30, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.551 [0.000, 1.000], mean observation: 0.189 [-0.441, 1.225], \n",
      " 14620/50000: episode: 334, duration: 1.264s, episode steps: 38, steps per second: 30, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.447 [0.000, 1.000], mean observation: -0.176 [-1.050, 0.384], \n",
      " 14743/50000: episode: 335, duration: 4.101s, episode steps: 123, steps per second: 30, episode reward: 123.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.113 [-0.612, 1.247], \n",
      " 14901/50000: episode: 336, duration: 5.264s, episode steps: 158, steps per second: 30, episode reward: 158.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.374 [-2.369, 0.482], \n",
      " 14938/50000: episode: 337, duration: 1.231s, episode steps: 37, steps per second: 30, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.541 [0.000, 1.000], mean observation: 0.098 [-0.274, 1.013], \n",
      " 15080/50000: episode: 338, duration: 4.734s, episode steps: 142, steps per second: 30, episode reward: 142.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.416 [-2.539, 0.546], \n",
      " 15207/50000: episode: 339, duration: 4.234s, episode steps: 127, steps per second: 30, episode reward: 127.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.116 [-0.401, 1.287], \n",
      " 15321/50000: episode: 340, duration: 3.797s, episode steps: 114, steps per second: 30, episode reward: 114.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.439 [0.000, 1.000], mean observation: -0.497 [-2.527, 0.513], \n",
      " 15455/50000: episode: 341, duration: 4.465s, episode steps: 134, steps per second: 30, episode reward: 134.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.436 [-2.350, 0.430], \n",
      " 15585/50000: episode: 342, duration: 4.334s, episode steps: 130, steps per second: 30, episode reward: 130.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.141 [-0.552, 1.517], \n",
      " 15760/50000: episode: 343, duration: 5.831s, episode steps: 175, steps per second: 30, episode reward: 175.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.358 [-2.395, 0.352], \n",
      " 15836/50000: episode: 344, duration: 2.531s, episode steps: 76, steps per second: 30, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.539 [0.000, 1.000], mean observation: 0.228 [-0.325, 1.293], \n",
      " 15948/50000: episode: 345, duration: 3.715s, episode steps: 112, steps per second: 30, episode reward: 112.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: -0.470 [-2.575, 0.667], \n",
      " 16081/50000: episode: 346, duration: 4.434s, episode steps: 133, steps per second: 30, episode reward: 133.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.451 [0.000, 1.000], mean observation: -0.365 [-2.327, 0.438], \n",
      " 16285/50000: episode: 347, duration: 6.799s, episode steps: 204, steps per second: 30, episode reward: 204.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.326 [-0.436, 2.414], \n",
      " 16299/50000: episode: 348, duration: 0.463s, episode steps: 14, steps per second: 30, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.130 [-1.031, 0.557], \n",
      " 16459/50000: episode: 349, duration: 5.335s, episode steps: 160, steps per second: 30, episode reward: 160.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.448 [-2.422, 0.587], \n",
      " 16470/50000: episode: 350, duration: 0.363s, episode steps: 11, steps per second: 30, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.113 [-0.980, 1.657], \n",
      " 16479/50000: episode: 351, duration: 0.300s, episode steps: 9, steps per second: 30, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.165 [-1.757, 2.895], \n",
      " 16492/50000: episode: 352, duration: 0.436s, episode steps: 13, steps per second: 30, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.088 [-1.552, 2.318], \n",
      " 16504/50000: episode: 353, duration: 0.399s, episode steps: 12, steps per second: 30, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.085 [-0.828, 1.244], \n",
      " 16532/50000: episode: 354, duration: 0.931s, episode steps: 28, steps per second: 30, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: 0.136 [-0.429, 0.708], \n",
      " 16600/50000: episode: 355, duration: 2.265s, episode steps: 68, steps per second: 30, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: 0.002 [-0.831, 0.586], \n",
      " 16709/50000: episode: 356, duration: 3.632s, episode steps: 109, steps per second: 30, episode reward: 109.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: -0.487 [-2.548, 0.559], \n",
      " 16745/50000: episode: 357, duration: 1.199s, episode steps: 36, steps per second: 30, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: -0.127 [-0.727, 0.396], \n",
      " 16823/50000: episode: 358, duration: 2.599s, episode steps: 78, steps per second: 30, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.423 [0.000, 1.000], mean observation: -0.378 [-2.178, 0.457], \n",
      " 16872/50000: episode: 359, duration: 1.632s, episode steps: 49, steps per second: 30, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.449 [0.000, 1.000], mean observation: -0.142 [-0.935, 0.368], \n",
      " 17018/50000: episode: 360, duration: 4.866s, episode steps: 146, steps per second: 30, episode reward: 146.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.103 [-1.810, 0.591], \n",
      " 17061/50000: episode: 361, duration: 1.432s, episode steps: 43, steps per second: 30, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.442 [0.000, 1.000], mean observation: -0.186 [-1.113, 0.377], \n",
      " 17114/50000: episode: 362, duration: 1.766s, episode steps: 53, steps per second: 30, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.453 [0.000, 1.000], mean observation: -0.068 [-0.879, 0.435], \n",
      " 17171/50000: episode: 363, duration: 1.899s, episode steps: 57, steps per second: 30, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.142 [-0.955, 0.379], \n",
      " 17236/50000: episode: 364, duration: 2.165s, episode steps: 65, steps per second: 30, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.446 [0.000, 1.000], mean observation: -0.165 [-1.275, 0.417], \n",
      " 17301/50000: episode: 365, duration: 2.167s, episode steps: 65, steps per second: 30, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.446 [0.000, 1.000], mean observation: -0.159 [-1.279, 0.385], \n",
      " 17372/50000: episode: 366, duration: 2.366s, episode steps: 71, steps per second: 30, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.451 [0.000, 1.000], mean observation: -0.129 [-1.251, 0.225], \n",
      " 17446/50000: episode: 367, duration: 2.467s, episode steps: 74, steps per second: 30, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.446 [0.000, 1.000], mean observation: -0.151 [-1.413, 0.437], \n",
      " 17506/50000: episode: 368, duration: 1.998s, episode steps: 60, steps per second: 30, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: -0.149 [-1.121, 0.360], \n",
      " 17555/50000: episode: 369, duration: 1.633s, episode steps: 49, steps per second: 30, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: -0.182 [-1.315, 0.360], \n",
      " 17607/50000: episode: 370, duration: 1.732s, episode steps: 52, steps per second: 30, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.442 [0.000, 1.000], mean observation: -0.174 [-1.111, 0.378], \n",
      " 17656/50000: episode: 371, duration: 1.632s, episode steps: 49, steps per second: 30, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.449 [0.000, 1.000], mean observation: -0.114 [-0.906, 0.398], \n",
      " 17703/50000: episode: 372, duration: 1.567s, episode steps: 47, steps per second: 30, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.447 [0.000, 1.000], mean observation: -0.134 [-0.950, 0.357], \n",
      " 17757/50000: episode: 373, duration: 1.800s, episode steps: 54, steps per second: 30, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: -0.197 [-1.127, 0.358], \n",
      " 17800/50000: episode: 374, duration: 1.432s, episode steps: 43, steps per second: 30, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.193 [-0.947, 0.169], \n",
      " 17863/50000: episode: 375, duration: 2.100s, episode steps: 63, steps per second: 30, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: -0.179 [-1.260, 0.616], \n",
      " 17912/50000: episode: 376, duration: 1.632s, episode steps: 49, steps per second: 30, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.449 [0.000, 1.000], mean observation: -0.147 [-0.917, 0.380], \n",
      " 17973/50000: episode: 377, duration: 2.032s, episode steps: 61, steps per second: 30, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.443 [0.000, 1.000], mean observation: -0.178 [-1.305, 0.356], \n",
      " 18027/50000: episode: 378, duration: 1.799s, episode steps: 54, steps per second: 30, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.138 [-0.910, 0.392], \n",
      " 18078/50000: episode: 379, duration: 1.700s, episode steps: 51, steps per second: 30, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.451 [0.000, 1.000], mean observation: -0.166 [-1.078, 0.420], \n",
      " 18127/50000: episode: 380, duration: 1.632s, episode steps: 49, steps per second: 30, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.449 [0.000, 1.000], mean observation: -0.140 [-0.945, 0.321], \n",
      " 18173/50000: episode: 381, duration: 1.530s, episode steps: 46, steps per second: 30, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.435 [0.000, 1.000], mean observation: -0.188 [-1.087, 0.236], \n",
      " 18211/50000: episode: 382, duration: 1.254s, episode steps: 38, steps per second: 30, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.447 [0.000, 1.000], mean observation: -0.135 [-0.780, 0.383], \n",
      " 18268/50000: episode: 383, duration: 1.897s, episode steps: 57, steps per second: 30, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.439 [0.000, 1.000], mean observation: -0.157 [-1.237, 0.429], \n",
      " 18323/50000: episode: 384, duration: 1.832s, episode steps: 55, steps per second: 30, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.191 [-1.065, 0.426], \n",
      " 18372/50000: episode: 385, duration: 1.634s, episode steps: 49, steps per second: 30, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.449 [0.000, 1.000], mean observation: -0.133 [-0.887, 0.415], \n",
      " 18445/50000: episode: 386, duration: 2.432s, episode steps: 73, steps per second: 30, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.128 [-1.058, 0.425], \n",
      " 18505/50000: episode: 387, duration: 1.999s, episode steps: 60, steps per second: 30, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: -0.104 [-1.065, 0.412], \n",
      " 18546/50000: episode: 388, duration: 1.366s, episode steps: 41, steps per second: 30, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.439 [0.000, 1.000], mean observation: -0.126 [-0.900, 0.401], \n",
      " 18584/50000: episode: 389, duration: 1.268s, episode steps: 38, steps per second: 30, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.447 [0.000, 1.000], mean observation: -0.133 [-0.869, 0.610], \n",
      " 18655/50000: episode: 390, duration: 2.363s, episode steps: 71, steps per second: 30, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.108 [-0.970, 0.941], \n",
      " 18725/50000: episode: 391, duration: 2.332s, episode steps: 70, steps per second: 30, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.160 [-0.894, 0.361], \n",
      " 18781/50000: episode: 392, duration: 1.867s, episode steps: 56, steps per second: 30, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.187 [-1.166, 0.613], \n",
      " 18862/50000: episode: 393, duration: 2.699s, episode steps: 81, steps per second: 30, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.148 [-1.150, 0.352], \n",
      " 18934/50000: episode: 394, duration: 2.399s, episode steps: 72, steps per second: 30, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.100 [-1.089, 0.519], \n",
      " 18992/50000: episode: 395, duration: 1.932s, episode steps: 58, steps per second: 30, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.154 [-1.125, 0.255], \n",
      " 19046/50000: episode: 396, duration: 1.800s, episode steps: 54, steps per second: 30, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.198 [-0.963, 0.369], \n",
      " 19170/50000: episode: 397, duration: 4.133s, episode steps: 124, steps per second: 30, episode reward: 124.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.129 [-1.072, 0.447], \n",
      " 19262/50000: episode: 398, duration: 3.066s, episode steps: 92, steps per second: 30, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.123 [-1.150, 0.535], \n",
      " 19378/50000: episode: 399, duration: 3.865s, episode steps: 116, steps per second: 30, episode reward: 116.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.146 [-1.140, 0.545], \n",
      " 19542/50000: episode: 400, duration: 5.466s, episode steps: 164, steps per second: 30, episode reward: 164.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.137 [-1.159, 0.581], \n",
      " 19751/50000: episode: 401, duration: 6.964s, episode steps: 209, steps per second: 30, episode reward: 209.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.376 [-0.566, 2.409], \n",
      " 19887/50000: episode: 402, duration: 4.532s, episode steps: 136, steps per second: 30, episode reward: 136.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.214 [-1.119, 0.595], \n",
      " 20007/50000: episode: 403, duration: 3.999s, episode steps: 120, steps per second: 30, episode reward: 120.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.470 [-0.645, 2.401], \n",
      " 20144/50000: episode: 404, duration: 4.565s, episode steps: 137, steps per second: 30, episode reward: 137.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.044 [-1.271, 1.313], \n",
      " 20413/50000: episode: 405, duration: 8.965s, episode steps: 269, steps per second: 30, episode reward: 269.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.352 [-2.415, 0.531], \n",
      " 20449/50000: episode: 406, duration: 1.199s, episode steps: 36, steps per second: 30, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: 0.175 [-0.346, 0.776], \n",
      " 20511/50000: episode: 407, duration: 2.065s, episode steps: 62, steps per second: 30, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.548 [0.000, 1.000], mean observation: 0.163 [-0.365, 1.117], \n",
      " 20569/50000: episode: 408, duration: 1.934s, episode steps: 58, steps per second: 30, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.552 [0.000, 1.000], mean observation: 0.145 [-0.331, 1.081], \n",
      " 20601/50000: episode: 409, duration: 1.065s, episode steps: 32, steps per second: 30, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.082 [-0.548, 0.811], \n",
      " 20642/50000: episode: 410, duration: 1.366s, episode steps: 41, steps per second: 30, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.561 [0.000, 1.000], mean observation: 0.166 [-0.349, 0.950], \n",
      " 20712/50000: episode: 411, duration: 2.332s, episode steps: 70, steps per second: 30, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.084 [-0.615, 0.875], \n",
      " 20740/50000: episode: 412, duration: 0.933s, episode steps: 28, steps per second: 30, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: 0.104 [-0.407, 0.684], \n",
      " 20765/50000: episode: 413, duration: 0.832s, episode steps: 25, steps per second: 30, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.560 [0.000, 1.000], mean observation: 0.099 [-0.596, 0.954], \n",
      " 20810/50000: episode: 414, duration: 1.500s, episode steps: 45, steps per second: 30, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: 0.162 [-0.241, 0.947], \n",
      " 20865/50000: episode: 415, duration: 1.833s, episode steps: 55, steps per second: 30, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.120 [-0.579, 0.908], \n",
      " 20911/50000: episode: 416, duration: 1.534s, episode steps: 46, steps per second: 30, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.543 [0.000, 1.000], mean observation: 0.140 [-0.363, 0.768], \n",
      " 20935/50000: episode: 417, duration: 0.798s, episode steps: 24, steps per second: 30, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: 0.122 [-0.583, 0.941], \n",
      " 20966/50000: episode: 418, duration: 1.032s, episode steps: 31, steps per second: 30, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.548 [0.000, 1.000], mean observation: 0.100 [-0.403, 0.930], \n",
      " 21008/50000: episode: 419, duration: 1.432s, episode steps: 42, steps per second: 29, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.548 [0.000, 1.000], mean observation: 0.124 [-0.417, 0.701], \n",
      " 21051/50000: episode: 420, duration: 1.433s, episode steps: 43, steps per second: 30, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.535 [0.000, 1.000], mean observation: 0.135 [-0.569, 0.853], \n",
      " 21105/50000: episode: 421, duration: 1.798s, episode steps: 54, steps per second: 30, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: 0.103 [-0.423, 0.873], \n",
      " 21138/50000: episode: 422, duration: 1.100s, episode steps: 33, steps per second: 30, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.103 [-0.348, 0.856], \n",
      " 21182/50000: episode: 423, duration: 1.466s, episode steps: 44, steps per second: 30, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.114 [-0.411, 0.708], \n",
      " 21220/50000: episode: 424, duration: 1.266s, episode steps: 38, steps per second: 30, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.553 [0.000, 1.000], mean observation: 0.178 [-0.189, 0.744], \n",
      " 21269/50000: episode: 425, duration: 1.633s, episode steps: 49, steps per second: 30, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.551 [0.000, 1.000], mean observation: 0.148 [-0.297, 0.862], \n",
      " 21328/50000: episode: 426, duration: 1.966s, episode steps: 59, steps per second: 30, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: 0.149 [-0.374, 0.906], \n",
      " 21515/50000: episode: 427, duration: 6.233s, episode steps: 187, steps per second: 30, episode reward: 187.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.064 [-0.494, 1.075], \n",
      " 21616/50000: episode: 428, duration: 3.365s, episode steps: 101, steps per second: 30, episode reward: 101.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.183 [-0.456, 1.070], \n",
      " 21778/50000: episode: 429, duration: 5.399s, episode steps: 162, steps per second: 30, episode reward: 162.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.306 [-0.427, 1.647], \n",
      " 21800/50000: episode: 430, duration: 0.733s, episode steps: 22, steps per second: 30, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.074 [-0.886, 0.581], \n",
      " 21954/50000: episode: 431, duration: 5.131s, episode steps: 154, steps per second: 30, episode reward: 154.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.491 [-2.408, 0.830], \n",
      " 21966/50000: episode: 432, duration: 0.399s, episode steps: 12, steps per second: 30, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.085 [-1.006, 1.588], \n",
      " 21979/50000: episode: 433, duration: 0.432s, episode steps: 13, steps per second: 30, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.089 [-0.814, 1.228], \n",
      " 21998/50000: episode: 434, duration: 0.633s, episode steps: 19, steps per second: 30, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.111 [-0.598, 0.947], \n",
      " 22048/50000: episode: 435, duration: 1.665s, episode steps: 50, steps per second: 30, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.560 [0.000, 1.000], mean observation: 0.207 [-0.614, 1.251], \n",
      " 22107/50000: episode: 436, duration: 1.967s, episode steps: 59, steps per second: 30, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.559 [0.000, 1.000], mean observation: 0.276 [-0.212, 1.322], \n",
      " 22215/50000: episode: 437, duration: 3.600s, episode steps: 108, steps per second: 30, episode reward: 108.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: 0.278 [-0.527, 1.440], \n",
      " 22564/50000: episode: 438, duration: 11.613s, episode steps: 349, steps per second: 30, episode reward: 349.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.236 [-0.895, 2.419], \n",
      " 22583/50000: episode: 439, duration: 0.631s, episode steps: 19, steps per second: 30, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.059 [-1.384, 0.995], \n",
      " 22761/50000: episode: 440, duration: 5.933s, episode steps: 178, steps per second: 30, episode reward: 178.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.477 [-2.408, 0.763], \n",
      " 22781/50000: episode: 441, duration: 0.667s, episode steps: 20, steps per second: 30, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: 0.107 [-0.608, 1.032], \n",
      " 22853/50000: episode: 442, duration: 2.398s, episode steps: 72, steps per second: 30, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.569 [0.000, 1.000], mean observation: 0.308 [-0.595, 1.827], \n",
      " 22933/50000: episode: 443, duration: 2.665s, episode steps: 80, steps per second: 30, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: 0.335 [-0.593, 1.826], \n",
      " 23008/50000: episode: 444, duration: 2.499s, episode steps: 75, steps per second: 30, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.573 [0.000, 1.000], mean observation: 0.295 [-0.593, 1.990], \n",
      " 23042/50000: episode: 445, duration: 1.133s, episode steps: 34, steps per second: 30, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.108 [-0.608, 0.862], \n",
      " 23064/50000: episode: 446, duration: 0.734s, episode steps: 22, steps per second: 30, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.112 [-0.562, 0.912], \n",
      " 23076/50000: episode: 447, duration: 0.399s, episode steps: 12, steps per second: 30, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.128 [-0.759, 1.287], \n",
      " 23085/50000: episode: 448, duration: 0.299s, episode steps: 9, steps per second: 30, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.120 [-1.009, 1.593], \n",
      " 23095/50000: episode: 449, duration: 0.333s, episode steps: 10, steps per second: 30, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.122 [-0.948, 1.541], \n",
      " 23107/50000: episode: 450, duration: 0.399s, episode steps: 12, steps per second: 30, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.096 [-1.378, 2.087], \n",
      " 23122/50000: episode: 451, duration: 0.500s, episode steps: 15, steps per second: 30, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.117 [-0.744, 1.265], \n",
      " 23222/50000: episode: 452, duration: 3.332s, episode steps: 100, steps per second: 30, episode reward: 100.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.570 [0.000, 1.000], mean observation: 0.556 [-0.624, 2.516], \n",
      " 23443/50000: episode: 453, duration: 7.365s, episode steps: 221, steps per second: 30, episode reward: 221.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.278 [-2.404, 0.585], \n",
      " 23573/50000: episode: 454, duration: 4.333s, episode steps: 130, steps per second: 30, episode reward: 130.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.454 [-2.414, 0.584], \n",
      " 23682/50000: episode: 455, duration: 3.631s, episode steps: 109, steps per second: 30, episode reward: 109.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: -0.496 [-2.413, 0.636], \n",
      " 23708/50000: episode: 456, duration: 0.865s, episode steps: 26, steps per second: 30, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.108 [-0.893, 0.346], \n",
      " 23837/50000: episode: 457, duration: 4.299s, episode steps: 129, steps per second: 30, episode reward: 129.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.457 [-2.437, 0.782], \n",
      " 23967/50000: episode: 458, duration: 4.333s, episode steps: 130, steps per second: 30, episode reward: 130.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.434 [-2.429, 0.633], \n",
      " 24129/50000: episode: 459, duration: 5.398s, episode steps: 162, steps per second: 30, episode reward: 162.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.354 [-2.403, 0.498], \n",
      " 24266/50000: episode: 460, duration: 4.565s, episode steps: 137, steps per second: 30, episode reward: 137.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.429 [-2.406, 0.668], \n",
      " 24401/50000: episode: 461, duration: 4.500s, episode steps: 135, steps per second: 30, episode reward: 135.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.459 [-2.409, 0.743], \n",
      " 24525/50000: episode: 462, duration: 4.131s, episode steps: 124, steps per second: 30, episode reward: 124.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.497 [-2.406, 0.806], \n",
      " 24662/50000: episode: 463, duration: 4.566s, episode steps: 137, steps per second: 30, episode reward: 137.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.407 [-2.405, 0.634], \n",
      " 24772/50000: episode: 464, duration: 3.665s, episode steps: 110, steps per second: 30, episode reward: 110.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.515 [-2.414, 0.606], \n",
      " 24891/50000: episode: 465, duration: 3.965s, episode steps: 119, steps per second: 30, episode reward: 119.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.471 [-2.424, 0.657], \n",
      " 25005/50000: episode: 466, duration: 3.799s, episode steps: 114, steps per second: 30, episode reward: 114.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.501 [-2.417, 0.507], \n",
      " 25120/50000: episode: 467, duration: 3.832s, episode steps: 115, steps per second: 30, episode reward: 115.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.515 [-2.418, 0.597], \n",
      " 25259/50000: episode: 468, duration: 4.632s, episode steps: 139, steps per second: 30, episode reward: 139.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.453 [-2.424, 0.576], \n",
      " 25383/50000: episode: 469, duration: 4.133s, episode steps: 124, steps per second: 30, episode reward: 124.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.471 [-2.432, 0.465], \n",
      " 25501/50000: episode: 470, duration: 3.931s, episode steps: 118, steps per second: 30, episode reward: 118.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.484 [-2.408, 0.773], \n",
      " 25615/50000: episode: 471, duration: 3.798s, episode steps: 114, steps per second: 30, episode reward: 114.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.482 [-2.416, 0.666], \n",
      " 25754/50000: episode: 472, duration: 4.634s, episode steps: 139, steps per second: 30, episode reward: 139.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.448 [-2.402, 0.650], \n",
      " 25778/50000: episode: 473, duration: 0.799s, episode steps: 24, steps per second: 30, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.073 [-0.933, 0.448], \n",
      " 25800/50000: episode: 474, duration: 0.732s, episode steps: 22, steps per second: 30, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.047 [-0.895, 0.628], \n",
      " 26238/50000: episode: 475, duration: 14.584s, episode steps: 438, steps per second: 30, episode reward: 438.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.275 [-0.856, 2.408], \n",
      " 26273/50000: episode: 476, duration: 1.163s, episode steps: 35, steps per second: 30, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.543 [0.000, 1.000], mean observation: 0.093 [-0.544, 0.875], \n",
      " 26388/50000: episode: 477, duration: 3.832s, episode steps: 115, steps per second: 30, episode reward: 115.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.557 [0.000, 1.000], mean observation: 0.423 [-0.409, 2.349], \n",
      " 26515/50000: episode: 478, duration: 4.232s, episode steps: 127, steps per second: 30, episode reward: 127.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.551 [0.000, 1.000], mean observation: 0.482 [-0.442, 2.431], \n",
      " 26641/50000: episode: 479, duration: 4.199s, episode steps: 126, steps per second: 30, episode reward: 126.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.548 [0.000, 1.000], mean observation: 0.449 [-0.569, 2.378], \n",
      " 26819/50000: episode: 480, duration: 5.931s, episode steps: 178, steps per second: 30, episode reward: 178.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.377 [-0.586, 2.406], \n",
      " 26979/50000: episode: 481, duration: 5.332s, episode steps: 160, steps per second: 30, episode reward: 160.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.419 [-0.491, 2.402], \n",
      " 26999/50000: episode: 482, duration: 0.667s, episode steps: 20, steps per second: 30, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: -0.099 [-0.973, 0.591], \n",
      " 27235/50000: episode: 483, duration: 7.864s, episode steps: 236, steps per second: 30, episode reward: 236.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.334 [-0.770, 2.406], \n",
      " 27407/50000: episode: 484, duration: 5.730s, episode steps: 172, steps per second: 30, episode reward: 172.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.416 [-0.508, 2.400], \n",
      " 27669/50000: episode: 485, duration: 8.733s, episode steps: 262, steps per second: 30, episode reward: 262.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.387 [-0.729, 2.416], \n",
      " 27983/50000: episode: 486, duration: 10.465s, episode steps: 314, steps per second: 30, episode reward: 314.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: 0.388 [-0.604, 2.410], \n",
      " 28169/50000: episode: 487, duration: 6.198s, episode steps: 186, steps per second: 30, episode reward: 186.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.438 [-2.408, 0.764], \n",
      " 28669/50000: episode: 488, duration: 16.663s, episode steps: 500, steps per second: 30, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.502 [0.000, 1.000], mean observation: 0.336 [-0.700, 2.341], \n",
      " 29155/50000: episode: 489, duration: 16.196s, episode steps: 486, steps per second: 30, episode reward: 486.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.391 [-0.898, 2.402], \n",
      " 29214/50000: episode: 490, duration: 1.966s, episode steps: 59, steps per second: 30, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.441 [0.000, 1.000], mean observation: -0.230 [-1.682, 1.106], \n",
      " 29227/50000: episode: 491, duration: 0.433s, episode steps: 13, steps per second: 30, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.082 [-0.980, 1.564], \n",
      " 29238/50000: episode: 492, duration: 0.365s, episode steps: 11, steps per second: 30, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.121 [-1.129, 1.741], \n",
      " 29250/50000: episode: 493, duration: 0.401s, episode steps: 12, steps per second: 30, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.110 [-0.967, 1.600], \n",
      " 29327/50000: episode: 494, duration: 2.565s, episode steps: 77, steps per second: 30, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: 0.158 [-0.976, 1.370], \n",
      " 29472/50000: episode: 495, duration: 4.832s, episode steps: 145, steps per second: 30, episode reward: 145.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.408 [-2.411, 0.597], \n",
      " 29489/50000: episode: 496, duration: 0.566s, episode steps: 17, steps per second: 30, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.099 [-1.186, 0.763], \n",
      " 29619/50000: episode: 497, duration: 4.316s, episode steps: 130, steps per second: 30, episode reward: 130.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.485 [-2.421, 0.635], \n",
      " 29764/50000: episode: 498, duration: 4.832s, episode steps: 145, steps per second: 30, episode reward: 145.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.436 [-2.404, 0.478], \n",
      " 29911/50000: episode: 499, duration: 4.898s, episode steps: 147, steps per second: 30, episode reward: 147.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.428 [-2.407, 0.787], \n",
      " 30059/50000: episode: 500, duration: 4.932s, episode steps: 148, steps per second: 30, episode reward: 148.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.423 [-2.404, 0.653], \n",
      " 30186/50000: episode: 501, duration: 4.233s, episode steps: 127, steps per second: 30, episode reward: 127.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.481 [-2.412, 0.575], \n",
      " 30315/50000: episode: 502, duration: 4.298s, episode steps: 129, steps per second: 30, episode reward: 129.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.464 [-2.416, 0.741], \n",
      " 30453/50000: episode: 503, duration: 4.598s, episode steps: 138, steps per second: 30, episode reward: 138.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.455 [-2.402, 0.824], \n",
      " 30589/50000: episode: 504, duration: 4.533s, episode steps: 136, steps per second: 30, episode reward: 136.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.457 [-2.425, 0.621], \n",
      " 30719/50000: episode: 505, duration: 4.332s, episode steps: 130, steps per second: 30, episode reward: 130.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.461 [-2.405, 0.401], \n",
      " 30852/50000: episode: 506, duration: 4.432s, episode steps: 133, steps per second: 30, episode reward: 133.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.470 [-2.428, 0.661], \n",
      " 30988/50000: episode: 507, duration: 4.532s, episode steps: 136, steps per second: 30, episode reward: 136.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.451 [-2.405, 0.434], \n",
      " 31131/50000: episode: 508, duration: 4.766s, episode steps: 143, steps per second: 30, episode reward: 143.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.453 [-2.426, 0.449], \n",
      " 31276/50000: episode: 509, duration: 4.833s, episode steps: 145, steps per second: 30, episode reward: 145.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.432 [-2.428, 0.567], \n",
      " 31430/50000: episode: 510, duration: 5.131s, episode steps: 154, steps per second: 30, episode reward: 154.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.406 [-2.407, 0.632], \n",
      " 31581/50000: episode: 511, duration: 5.032s, episode steps: 151, steps per second: 30, episode reward: 151.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.438 [-2.425, 0.437], \n",
      " 31723/50000: episode: 512, duration: 4.734s, episode steps: 142, steps per second: 30, episode reward: 142.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.371 [-2.009, 0.615], \n",
      " 31853/50000: episode: 513, duration: 4.330s, episode steps: 130, steps per second: 30, episode reward: 130.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.344 [-1.702, 0.511], \n",
      " 32012/50000: episode: 514, duration: 5.300s, episode steps: 159, steps per second: 30, episode reward: 159.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.429 [-2.434, 0.429], \n",
      " 32150/50000: episode: 515, duration: 4.598s, episode steps: 138, steps per second: 30, episode reward: 138.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.343 [-2.036, 0.647], \n",
      " 32270/50000: episode: 516, duration: 3.999s, episode steps: 120, steps per second: 30, episode reward: 120.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.322 [-1.515, 0.626], \n",
      " 32395/50000: episode: 517, duration: 4.165s, episode steps: 125, steps per second: 30, episode reward: 125.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.317 [-1.591, 0.702], \n",
      " 32576/50000: episode: 518, duration: 6.032s, episode steps: 181, steps per second: 30, episode reward: 181.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.431 [-2.401, 0.734], \n",
      " 32612/50000: episode: 519, duration: 1.198s, episode steps: 36, steps per second: 30, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: 0.100 [-1.697, 1.762], \n",
      " 32627/50000: episode: 520, duration: 0.499s, episode steps: 15, steps per second: 30, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.110 [-1.230, 0.785], \n",
      " 32917/50000: episode: 521, duration: 9.667s, episode steps: 290, steps per second: 30, episode reward: 290.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: 0.344 [-0.659, 2.401], \n",
      " 32947/50000: episode: 522, duration: 0.998s, episode steps: 30, steps per second: 30, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.567 [0.000, 1.000], mean observation: 0.119 [-0.542, 0.862], \n",
      " 33272/50000: episode: 523, duration: 10.830s, episode steps: 325, steps per second: 30, episode reward: 325.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.502 [0.000, 1.000], mean observation: 0.457 [-0.783, 2.401], \n",
      " 33772/50000: episode: 524, duration: 16.649s, episode steps: 500, steps per second: 30, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.502 [0.000, 1.000], mean observation: 0.346 [-0.765, 2.360], \n",
      " 34272/50000: episode: 525, duration: 16.662s, episode steps: 500, steps per second: 30, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.502 [0.000, 1.000], mean observation: 0.273 [-0.918, 1.826], \n",
      " 34772/50000: episode: 526, duration: 16.663s, episode steps: 500, steps per second: 30, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.217 [-0.604, 1.392], \n",
      " 35272/50000: episode: 527, duration: 16.662s, episode steps: 500, steps per second: 30, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.498 [0.000, 1.000], mean observation: 0.159 [-0.841, 1.101], \n",
      " 35467/50000: episode: 528, duration: 6.498s, episode steps: 195, steps per second: 30, episode reward: 195.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.412 [-2.400, 0.634], \n",
      " 35967/50000: episode: 529, duration: 16.665s, episode steps: 500, steps per second: 30, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.250 [-0.609, 1.443], \n",
      " 36467/50000: episode: 530, duration: 16.663s, episode steps: 500, steps per second: 30, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.092 [-0.892, 0.913], \n",
      " 36967/50000: episode: 531, duration: 16.663s, episode steps: 500, steps per second: 30, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.502 [0.000, 1.000], mean observation: 0.177 [-0.761, 1.083], \n",
      " 37152/50000: episode: 532, duration: 6.162s, episode steps: 185, steps per second: 30, episode reward: 185.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.313 [-1.127, 2.411], \n",
      " 37162/50000: episode: 533, duration: 0.335s, episode steps: 10, steps per second: 30, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.135 [-1.538, 2.452], \n",
      " 37171/50000: episode: 534, duration: 0.300s, episode steps: 9, steps per second: 30, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.130 [-1.594, 2.477], \n",
      " 37182/50000: episode: 535, duration: 0.363s, episode steps: 11, steps per second: 30, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.108 [-1.157, 1.743], \n",
      " 37193/50000: episode: 536, duration: 0.368s, episode steps: 11, steps per second: 30, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.105 [-0.805, 1.302], \n",
      " 37355/50000: episode: 537, duration: 5.399s, episode steps: 162, steps per second: 30, episode reward: 162.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.219 [-1.966, 0.925], \n",
      " 37364/50000: episode: 538, duration: 0.299s, episode steps: 9, steps per second: 30, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.119 [-2.112, 1.354], \n",
      " 37374/50000: episode: 539, duration: 0.332s, episode steps: 10, steps per second: 30, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.121 [-2.084, 1.331], \n",
      " 37385/50000: episode: 540, duration: 0.368s, episode steps: 11, steps per second: 30, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.126 [-1.825, 1.187], \n",
      " 37395/50000: episode: 541, duration: 0.331s, episode steps: 10, steps per second: 30, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.126 [-1.328, 0.745], \n",
      " 37409/50000: episode: 542, duration: 0.468s, episode steps: 14, steps per second: 30, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.117 [-1.257, 0.772], \n",
      " 37592/50000: episode: 543, duration: 6.082s, episode steps: 183, steps per second: 30, episode reward: 183.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.414 [-2.418, 0.813], \n",
      " 38092/50000: episode: 544, duration: 16.664s, episode steps: 500, steps per second: 30, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.162 [-0.707, 0.923], \n",
      " 38592/50000: episode: 545, duration: 16.663s, episode steps: 500, steps per second: 30, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.343 [-0.596, 2.163], \n",
      " 39092/50000: episode: 546, duration: 16.663s, episode steps: 500, steps per second: 30, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.223 [-0.582, 1.198], \n",
      " 39284/50000: episode: 547, duration: 6.400s, episode steps: 192, steps per second: 30, episode reward: 192.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.323 [-2.411, 0.591], \n",
      " 39397/50000: episode: 548, duration: 3.763s, episode steps: 113, steps per second: 30, episode reward: 113.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.451 [0.000, 1.000], mean observation: -0.495 [-2.405, 0.498], \n",
      " 39518/50000: episode: 549, duration: 4.033s, episode steps: 121, steps per second: 30, episode reward: 121.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.477 [-2.416, 0.647], \n",
      " 39640/50000: episode: 550, duration: 4.065s, episode steps: 122, steps per second: 30, episode reward: 122.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.478 [-2.431, 0.595], \n",
      " 39764/50000: episode: 551, duration: 4.132s, episode steps: 124, steps per second: 30, episode reward: 124.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.460 [0.000, 1.000], mean observation: -0.479 [-2.434, 0.589], \n",
      " 39889/50000: episode: 552, duration: 4.166s, episode steps: 125, steps per second: 30, episode reward: 125.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.475 [-2.419, 0.584], \n",
      " 40016/50000: episode: 553, duration: 4.231s, episode steps: 127, steps per second: 30, episode reward: 127.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.486 [-2.421, 0.557], \n",
      " 40148/50000: episode: 554, duration: 4.398s, episode steps: 132, steps per second: 30, episode reward: 132.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.472 [-2.400, 0.720], \n",
      " 40298/50000: episode: 555, duration: 4.999s, episode steps: 150, steps per second: 30, episode reward: 150.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.444 [-2.407, 0.526], \n",
      " 40459/50000: episode: 556, duration: 5.366s, episode steps: 161, steps per second: 30, episode reward: 161.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.425 [-2.404, 0.492], \n",
      " 40632/50000: episode: 557, duration: 5.765s, episode steps: 173, steps per second: 30, episode reward: 173.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.374 [-2.413, 0.548], \n",
      " 40790/50000: episode: 558, duration: 5.266s, episode steps: 158, steps per second: 30, episode reward: 158.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.443 [-2.412, 0.651], \n",
      " 40971/50000: episode: 559, duration: 6.033s, episode steps: 181, steps per second: 30, episode reward: 181.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.416 [-2.408, 0.648], \n",
      " 41140/50000: episode: 560, duration: 5.629s, episode steps: 169, steps per second: 30, episode reward: 169.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.434 [-2.401, 0.585], \n",
      " 41323/50000: episode: 561, duration: 6.098s, episode steps: 183, steps per second: 30, episode reward: 183.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.433 [-2.403, 0.714], \n",
      " 41823/50000: episode: 562, duration: 16.650s, episode steps: 500, steps per second: 30, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.020 [-0.915, 0.727], \n",
      " 42323/50000: episode: 563, duration: 16.663s, episode steps: 500, steps per second: 30, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.110 [-1.088, 0.753], \n",
      " 42572/50000: episode: 564, duration: 8.298s, episode steps: 249, steps per second: 30, episode reward: 249.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.494 [0.000, 1.000], mean observation: -0.393 [-2.404, 1.043], \n",
      " 43072/50000: episode: 565, duration: 16.665s, episode steps: 500, steps per second: 30, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.152 [-0.848, 0.810], \n",
      " 43572/50000: episode: 566, duration: 16.662s, episode steps: 500, steps per second: 30, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.502 [0.000, 1.000], mean observation: 0.352 [-0.826, 2.259], \n",
      " 44072/50000: episode: 567, duration: 16.663s, episode steps: 500, steps per second: 30, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.088 [-1.538, 1.241], \n",
      " 44451/50000: episode: 568, duration: 12.632s, episode steps: 379, steps per second: 30, episode reward: 379.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.504 [0.000, 1.000], mean observation: 0.318 [-0.710, 2.407], \n",
      " 44855/50000: episode: 569, duration: 13.445s, episode steps: 404, steps per second: 30, episode reward: 404.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.365 [-0.816, 2.401], \n",
      " 45184/50000: episode: 570, duration: 10.965s, episode steps: 329, steps per second: 30, episode reward: 329.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.284 [-1.148, 2.401], \n",
      " 45195/50000: episode: 571, duration: 0.366s, episode steps: 11, steps per second: 30, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.090 [-1.023, 1.603], \n",
      " 45205/50000: episode: 572, duration: 0.332s, episode steps: 10, steps per second: 30, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.139 [-1.321, 2.178], \n",
      " 45431/50000: episode: 573, duration: 7.532s, episode steps: 226, steps per second: 30, episode reward: 226.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.226 [-2.428, 1.516], \n",
      " 45440/50000: episode: 574, duration: 0.299s, episode steps: 9, steps per second: 30, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.164 [-2.523, 1.566], \n",
      " 45452/50000: episode: 575, duration: 0.400s, episode steps: 12, steps per second: 30, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.081 [-1.965, 1.385], \n",
      " 45464/50000: episode: 576, duration: 0.399s, episode steps: 12, steps per second: 30, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.102 [-1.777, 1.215], \n",
      " 45783/50000: episode: 577, duration: 10.632s, episode steps: 319, steps per second: 30, episode reward: 319.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.215 [-0.950, 2.421], \n",
      " 46039/50000: episode: 578, duration: 8.531s, episode steps: 256, steps per second: 30, episode reward: 256.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.504 [0.000, 1.000], mean observation: 0.393 [-0.829, 2.405], \n",
      " 46539/50000: episode: 579, duration: 16.666s, episode steps: 500, steps per second: 30, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.364 [-0.628, 2.171], \n",
      " 47039/50000: episode: 580, duration: 16.661s, episode steps: 500, steps per second: 30, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.496 [0.000, 1.000], mean observation: 0.275 [-1.066, 1.457], \n",
      " 47223/50000: episode: 581, duration: 6.131s, episode steps: 184, steps per second: 30, episode reward: 184.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.405 [-2.415, 0.877], \n",
      " 47455/50000: episode: 582, duration: 7.732s, episode steps: 232, steps per second: 30, episode reward: 232.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.496 [0.000, 1.000], mean observation: -0.459 [-2.406, 1.085], \n",
      " 47746/50000: episode: 583, duration: 9.698s, episode steps: 291, steps per second: 30, episode reward: 291.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.393 [-2.406, 0.676], \n",
      " 48246/50000: episode: 584, duration: 16.663s, episode steps: 500, steps per second: 30, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.101 [-0.787, 1.318], \n",
      " 48423/50000: episode: 585, duration: 5.898s, episode steps: 177, steps per second: 30, episode reward: 177.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.333 [-0.826, 2.405], \n",
      " 48432/50000: episode: 586, duration: 0.302s, episode steps: 9, steps per second: 30, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.120 [-1.403, 2.167], \n",
      " 48447/50000: episode: 587, duration: 0.496s, episode steps: 15, steps per second: 30, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.267 [0.000, 1.000], mean observation: 0.080 [-1.536, 2.274], \n",
      " 48458/50000: episode: 588, duration: 0.366s, episode steps: 11, steps per second: 30, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.091 [-1.380, 2.029], \n",
      " 48469/50000: episode: 589, duration: 0.366s, episode steps: 11, steps per second: 30, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.115 [-1.192, 1.777], \n",
      " 48485/50000: episode: 590, duration: 0.534s, episode steps: 16, steps per second: 30, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.087 [-0.938, 1.404], \n",
      " 48697/50000: episode: 591, duration: 7.048s, episode steps: 212, steps per second: 30, episode reward: 212.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.300 [-2.416, 0.875], \n",
      " 48856/50000: episode: 592, duration: 5.298s, episode steps: 159, steps per second: 30, episode reward: 159.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.421 [-2.417, 0.816], \n",
      " 49002/50000: episode: 593, duration: 4.864s, episode steps: 146, steps per second: 30, episode reward: 146.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.477 [-2.410, 0.853], \n",
      " 49161/50000: episode: 594, duration: 5.299s, episode steps: 159, steps per second: 30, episode reward: 159.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.462 [-2.410, 0.579], \n",
      " 49349/50000: episode: 595, duration: 6.265s, episode steps: 188, steps per second: 30, episode reward: 188.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.356 [-2.404, 0.603], \n",
      " 49501/50000: episode: 596, duration: 5.067s, episode steps: 152, steps per second: 30, episode reward: 152.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.462 [-2.400, 0.565], \n",
      " 49690/50000: episode: 597, duration: 6.297s, episode steps: 189, steps per second: 30, episode reward: 189.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.379 [-2.413, 0.689], \n",
      " 49903/50000: episode: 598, duration: 7.098s, episode steps: 213, steps per second: 30, episode reward: 213.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: -0.459 [-2.402, 0.683], \n",
      "done, took 1666.626 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25298fbae08>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sarsa.fit(env, nb_steps=50000, visualize=True, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 100 episodes ...\n",
      "Episode 1: reward: 500.000, steps: 500\n",
      "Episode 2: reward: 500.000, steps: 500\n",
      "Episode 3: reward: 500.000, steps: 500\n",
      "Episode 4: reward: 500.000, steps: 500\n",
      "Episode 5: reward: 500.000, steps: 500\n",
      "Episode 6: reward: 500.000, steps: 500\n",
      "Episode 7: reward: 500.000, steps: 500\n",
      "Episode 8: reward: 500.000, steps: 500\n",
      "Episode 9: reward: 500.000, steps: 500\n",
      "Episode 10: reward: 500.000, steps: 500\n",
      "Episode 11: reward: 500.000, steps: 500\n",
      "Episode 12: reward: 500.000, steps: 500\n",
      "Episode 13: reward: 500.000, steps: 500\n",
      "Episode 14: reward: 500.000, steps: 500\n",
      "Episode 15: reward: 500.000, steps: 500\n",
      "Episode 16: reward: 500.000, steps: 500\n",
      "Episode 17: reward: 500.000, steps: 500\n",
      "Episode 18: reward: 500.000, steps: 500\n",
      "Episode 19: reward: 138.000, steps: 138\n",
      "Episode 20: reward: 141.000, steps: 141\n",
      "Episode 21: reward: 500.000, steps: 500\n",
      "Episode 22: reward: 500.000, steps: 500\n",
      "Episode 23: reward: 500.000, steps: 500\n",
      "Episode 24: reward: 500.000, steps: 500\n",
      "Episode 25: reward: 500.000, steps: 500\n",
      "Episode 26: reward: 500.000, steps: 500\n",
      "Episode 27: reward: 500.000, steps: 500\n",
      "Episode 28: reward: 500.000, steps: 500\n",
      "Episode 29: reward: 500.000, steps: 500\n",
      "Episode 30: reward: 500.000, steps: 500\n",
      "Episode 31: reward: 500.000, steps: 500\n",
      "Episode 32: reward: 500.000, steps: 500\n",
      "Episode 33: reward: 500.000, steps: 500\n",
      "Episode 34: reward: 500.000, steps: 500\n",
      "Episode 35: reward: 500.000, steps: 500\n",
      "Episode 36: reward: 500.000, steps: 500\n",
      "Episode 37: reward: 500.000, steps: 500\n",
      "Episode 38: reward: 500.000, steps: 500\n",
      "Episode 39: reward: 500.000, steps: 500\n",
      "Episode 40: reward: 500.000, steps: 500\n",
      "Episode 41: reward: 500.000, steps: 500\n",
      "Episode 42: reward: 500.000, steps: 500\n",
      "Episode 43: reward: 144.000, steps: 144\n",
      "Episode 44: reward: 500.000, steps: 500\n",
      "Episode 45: reward: 500.000, steps: 500\n",
      "Episode 46: reward: 500.000, steps: 500\n",
      "Episode 47: reward: 500.000, steps: 500\n",
      "Episode 48: reward: 500.000, steps: 500\n",
      "Episode 49: reward: 500.000, steps: 500\n",
      "Episode 50: reward: 500.000, steps: 500\n",
      "Episode 51: reward: 500.000, steps: 500\n",
      "Episode 52: reward: 500.000, steps: 500\n",
      "Episode 53: reward: 500.000, steps: 500\n",
      "Episode 54: reward: 500.000, steps: 500\n",
      "Episode 55: reward: 500.000, steps: 500\n",
      "Episode 56: reward: 500.000, steps: 500\n",
      "Episode 57: reward: 500.000, steps: 500\n",
      "Episode 58: reward: 500.000, steps: 500\n",
      "Episode 59: reward: 142.000, steps: 142\n",
      "Episode 60: reward: 154.000, steps: 154\n",
      "Episode 61: reward: 140.000, steps: 140\n",
      "Episode 62: reward: 500.000, steps: 500\n",
      "Episode 63: reward: 500.000, steps: 500\n",
      "Episode 64: reward: 500.000, steps: 500\n",
      "Episode 65: reward: 142.000, steps: 142\n",
      "Episode 66: reward: 500.000, steps: 500\n",
      "Episode 67: reward: 500.000, steps: 500\n",
      "Episode 68: reward: 500.000, steps: 500\n",
      "Episode 69: reward: 500.000, steps: 500\n",
      "Episode 70: reward: 500.000, steps: 500\n",
      "Episode 71: reward: 500.000, steps: 500\n",
      "Episode 72: reward: 500.000, steps: 500\n",
      "Episode 73: reward: 500.000, steps: 500\n",
      "Episode 74: reward: 500.000, steps: 500\n",
      "Episode 75: reward: 137.000, steps: 137\n",
      "Episode 76: reward: 500.000, steps: 500\n",
      "Episode 77: reward: 132.000, steps: 132\n",
      "Episode 78: reward: 500.000, steps: 500\n",
      "Episode 79: reward: 500.000, steps: 500\n",
      "Episode 80: reward: 500.000, steps: 500\n",
      "Episode 81: reward: 500.000, steps: 500\n",
      "Episode 82: reward: 500.000, steps: 500\n",
      "Episode 83: reward: 148.000, steps: 148\n",
      "Episode 84: reward: 500.000, steps: 500\n",
      "Episode 85: reward: 500.000, steps: 500\n",
      "Episode 86: reward: 500.000, steps: 500\n",
      "Episode 87: reward: 500.000, steps: 500\n",
      "Episode 88: reward: 500.000, steps: 500\n",
      "Episode 89: reward: 500.000, steps: 500\n",
      "Episode 90: reward: 500.000, steps: 500\n",
      "Episode 91: reward: 500.000, steps: 500\n",
      "Episode 92: reward: 500.000, steps: 500\n",
      "Episode 93: reward: 142.000, steps: 142\n",
      "Episode 94: reward: 138.000, steps: 138\n",
      "Episode 95: reward: 500.000, steps: 500\n",
      "Episode 96: reward: 500.000, steps: 500\n",
      "Episode 97: reward: 500.000, steps: 500\n",
      "Episode 98: reward: 500.000, steps: 500\n",
      "Episode 99: reward: 500.000, steps: 500\n",
      "Episode 100: reward: 500.000, steps: 500\n",
      "Average score over 100 test games:456.98\n"
     ]
    }
   ],
   "source": [
    "scores = sarsa.test(env, nb_episodes=100, visualize=True)\n",
    "print(\n",
    "    \"Average score over 100 test games:{}\".format(\n",
    "        np.mean(scores.history[\"episode_reward\"])\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarsa.save_weights(\"sarsa_cartpole.h5\", overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarsa.load_weights(\"sarsa_cartpole.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 50 episodes ...\n",
      "Episode 1: reward: 500.000, steps: 500\n",
      "Episode 2: reward: 500.000, steps: 500\n",
      "Episode 3: reward: 500.000, steps: 500\n",
      "Episode 4: reward: 500.000, steps: 500\n",
      "Episode 5: reward: 145.000, steps: 145\n",
      "Episode 6: reward: 500.000, steps: 500\n",
      "Episode 7: reward: 500.000, steps: 500\n",
      "Episode 8: reward: 500.000, steps: 500\n",
      "Episode 9: reward: 500.000, steps: 500\n",
      "Episode 10: reward: 500.000, steps: 500\n",
      "Episode 11: reward: 500.000, steps: 500\n",
      "Episode 12: reward: 500.000, steps: 500\n",
      "Episode 13: reward: 139.000, steps: 139\n",
      "Episode 14: reward: 500.000, steps: 500\n",
      "Episode 15: reward: 500.000, steps: 500\n",
      "Episode 16: reward: 132.000, steps: 132\n",
      "Episode 17: reward: 500.000, steps: 500\n",
      "Episode 18: reward: 500.000, steps: 500\n",
      "Episode 19: reward: 500.000, steps: 500\n",
      "Episode 20: reward: 500.000, steps: 500\n",
      "Episode 21: reward: 500.000, steps: 500\n",
      "Episode 22: reward: 500.000, steps: 500\n",
      "Episode 23: reward: 500.000, steps: 500\n",
      "Episode 24: reward: 500.000, steps: 500\n",
      "Episode 25: reward: 500.000, steps: 500\n",
      "Episode 26: reward: 500.000, steps: 500\n",
      "Episode 27: reward: 500.000, steps: 500\n",
      "Episode 28: reward: 500.000, steps: 500\n",
      "Episode 29: reward: 500.000, steps: 500\n",
      "Episode 30: reward: 500.000, steps: 500\n",
      "Episode 31: reward: 139.000, steps: 139\n",
      "Episode 32: reward: 500.000, steps: 500\n",
      "Episode 33: reward: 500.000, steps: 500\n",
      "Episode 34: reward: 500.000, steps: 500\n",
      "Episode 35: reward: 500.000, steps: 500\n",
      "Episode 36: reward: 142.000, steps: 142\n",
      "Episode 37: reward: 500.000, steps: 500\n",
      "Episode 38: reward: 500.000, steps: 500\n",
      "Episode 39: reward: 500.000, steps: 500\n",
      "Episode 40: reward: 500.000, steps: 500\n",
      "Episode 41: reward: 500.000, steps: 500\n",
      "Episode 42: reward: 500.000, steps: 500\n",
      "Episode 43: reward: 500.000, steps: 500\n",
      "Episode 44: reward: 500.000, steps: 500\n",
      "Episode 45: reward: 500.000, steps: 500\n",
      "Episode 46: reward: 500.000, steps: 500\n",
      "Episode 47: reward: 500.000, steps: 500\n",
      "Episode 48: reward: 500.000, steps: 500\n",
      "Episode 49: reward: 500.000, steps: 500\n",
      "Episode 50: reward: 500.000, steps: 500\n"
     ]
    }
   ],
   "source": [
    "_ = sarsa.test(env, nb_episodes=50, visualize=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
